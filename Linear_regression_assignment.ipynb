{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMu//xofwDTMdHVcbOIcoxf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dashnyam7/DIC_assignment/blob/main/Linear_regression_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Problem 1] Hypothetical function"
      ],
      "metadata": {
        "id": "BzOqcadknccr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "id": "6LavBL5knWzS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "class ScratchLinearRegression():\n",
        "    \"\"\"\n",
        "    Scratch implementation of linear regression\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    num_iter : int\n",
        "      number of iterations\n",
        "    lr : float      \n",
        "      learning rate\n",
        "    no_bias : bool\n",
        "      If we do not include the bias term True\n",
        "    verbose : bool\n",
        "      When outputting the learning process True\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    self.coef_ : ndarray, shape (n_features,)\n",
        "      parameter\n",
        "    self.loss : ndarray, shape (self.iter,)\n",
        "      Record loss on training data\n",
        "    self.val_loss : ndarray, shape (self.iter,)\n",
        "      Record Loss Against Validation Data\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
        "        # Record hyperparameters as attributes\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.no_bias = no_bias\n",
        "        self.verbose = verbose\n",
        "        # Prepare an array to record losses\n",
        "        self.loss = np.zeros(self.iter)\n",
        "        self.val_loss = np.zeros(self.iter)\n",
        "\n",
        "    def _linear_hypothesis(self, X):\n",
        "        \"\"\"   \n",
        "        compute the linear hypothesis function\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "          training data\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ndarray, shape (n_samples, 1)\n",
        "        Estimation result by linear hypothesis function\n",
        "\n",
        "        \"\"\"\n",
        "        pred = X @ self.theta\n",
        "        return pred\n",
        "        \n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        Learn linear regression. If validation data is input, the loss and accuracy for it are also calculated for each iteration.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            Features of training data\n",
        "        y : ndarray, shape (n_samples, )\n",
        "            Correct value of training data\n",
        "        X_val : ndarray, shape (n_samples, n_features)\n",
        "            Features of validation data\n",
        "        y_val : ndarray, shape (n_samples, )\n",
        "            Correct value of validation data\n",
        "        \"\"\"\n",
        "        if self.verbose:\n",
        "            #Output learning process when verbose is set to True\n",
        "            print()\n",
        "        pass\n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate using linear regression。\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            sample\n",
        "        Returns\n",
        "        -------\n",
        "            ndarray, shape (n_samples, 1)\n",
        "            Estimation result by linear regression\n",
        "        \"\"\"\n",
        "        pass\n",
        "        return\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Problem 2] The gradient descent method"
      ],
      "metadata": {
        "id": "pU8eObB3pYpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScratchLinearRegression():\n",
        "    \"\"\"\n",
        "    Scratch implementation of linear regression\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    num_iter : int\n",
        "      number of iterations\n",
        "    lr : float      \n",
        "      learning rate\n",
        "    no_bias : bool\n",
        "      If we do not include the bias term True\n",
        "    verbose : bool\n",
        "      When outputting the learning process True\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    self.coef_ : ndarray, shape (n_features,)\n",
        "      parameter\n",
        "    self.loss : ndarray, shape (self.iter,)\n",
        "      Record loss on training data\n",
        "    self.val_loss : ndarray, shape (self.iter,)\n",
        "      Record Loss Against Validation Data\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
        "        # Record hyperparameters as attributes\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.no_bias = no_bias\n",
        "        self.verbose = verbose\n",
        "        # Prepare an array to record losses\n",
        "        self.loss = np.zeros(self.iter)\n",
        "        self.val_loss = np.zeros(self.iter)\n",
        "\n",
        "    def _linear_hypothesis(self, X):\n",
        "        \"\"\"   \n",
        "        compute the linear hypothesis function\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "          training data\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ndarray, shape (n_samples, 1)\n",
        "        Estimation result by linear hypothesis function\n",
        "\n",
        "        \"\"\"\n",
        "        pred = X @ self.theta\n",
        "        return pred\n",
        "\n",
        "    def _gradient_descent(self, X, y):\n",
        "      \"\"\"\n",
        "      gradient descent algorithm, update formula\n",
        "      \"\"\"\n",
        "      m = X.shape[0]\n",
        "      n = X.shape[1]\n",
        "      pred = self._linear_hypothesis(X)\n",
        "      for j in range(n):\n",
        "          gradient = 0\n",
        "          for i in range(m):\n",
        "              gradient += (pred[i] - y[i]) * X[i, j]\n",
        "          self.theta[j] = self.theta[j] - self.lr * (gradient / m)    \n",
        "        \n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        Learn linear regression. If validation data is input, the loss and accuracy for it are also calculated for each iteration.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            Features of training data\n",
        "        y : ndarray, shape (n_samples, )\n",
        "            Correct value of training data\n",
        "        X_val : ndarray, shape (n_samples, n_features)\n",
        "            Features of validation data\n",
        "        y_val : ndarray, shape (n_samples, )\n",
        "            Correct value of validation data\n",
        "        \"\"\"\n",
        "        if self.verbose:\n",
        "            #Output learning process when verbose is set to True\n",
        "            print()\n",
        "        pass\n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate using linear regression。\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            sample\n",
        "        Returns\n",
        "        -------\n",
        "            ndarray, shape (n_samples, 1)\n",
        "            Estimation result by linear regression\n",
        "        \"\"\"\n",
        "        pass\n",
        "        return\n"
      ],
      "metadata": {
        "id": "TnAWsZgrpiuW"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Question 3] Presumption"
      ],
      "metadata": {
        "id": "vUWTFbnGpcvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScratchLinearRegression():\n",
        "    \"\"\"\n",
        "    Scratch implementation of linear regression\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    num_iter : int\n",
        "      number of iterations\n",
        "    lr : float      \n",
        "      learning rate\n",
        "    no_bias : bool\n",
        "      If we do not include the bias term True\n",
        "    verbose : bool\n",
        "      When outputting the learning process True\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    self.coef_ : ndarray, shape (n_features,)\n",
        "      parameter\n",
        "    self.loss : ndarray, shape (self.iter,)\n",
        "      Record loss on training data\n",
        "    self.val_loss : ndarray, shape (self.iter,)\n",
        "      Record Loss Against Validation Data\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
        "        # Record hyperparameters as attributes\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.no_bias = no_bias\n",
        "        self.verbose = verbose\n",
        "        # Prepare an array to record losses\n",
        "        self.loss = np.zeros(self.iter)\n",
        "        self.val_loss = np.zeros(self.iter)\n",
        "\n",
        "    def _linear_hypothesis(self, X):\n",
        "        \"\"\"   \n",
        "        compute the linear hypothesis function\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "          training data\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ndarray, shape (n_samples, 1)\n",
        "        Estimation result by linear hypothesis function\n",
        "\n",
        "        \"\"\"\n",
        "        pred = X @ self.theta\n",
        "        return pred\n",
        "\n",
        "    def _gradient_descent(self, X, y):\n",
        "      \"\"\"\n",
        "      gradient descent algorithm, update formula\n",
        "      \"\"\"\n",
        "      m = X.shape[0]\n",
        "      n = X.shape[1]\n",
        "      pred = self._linear_hypothesis(X)\n",
        "      for j in range(n):\n",
        "          gradient = 0\n",
        "          for i in range(m):\n",
        "              gradient += (pred[i] - y[i]) * X[i, j]\n",
        "          self.theta[j] = self.theta[j] - self.lr * (gradient / m)    \n",
        "        \n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        Learn linear regression. If validation data is input, the loss and accuracy for it are also calculated for each iteration.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            Features of training data\n",
        "        y : ndarray, shape (n_samples, )\n",
        "            Correct value of training data\n",
        "        X_val : ndarray, shape (n_samples, n_features)\n",
        "            Features of validation data\n",
        "        y_val : ndarray, shape (n_samples, )\n",
        "            Correct value of validation data\n",
        "        \"\"\"\n",
        "        if self.verbose:\n",
        "            #Output learning process when verbose is set to True\n",
        "            print()\n",
        "        pass\n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate using linear regression。\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            sample\n",
        "        Returns\n",
        "        -------\n",
        "            ndarray, shape (n_samples, 1)\n",
        "            Estimation result by linear regression\n",
        "        \"\"\"\n",
        "        if self.bias == True:\n",
        "            bias = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
        "            X = np.hstack([bias, X])\n",
        "        pred_y = self._linear_hypothesis(X)\n",
        "        return pred_y"
      ],
      "metadata": {
        "id": "rXT9Pkh8rDTN"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Question 4]  Mean squared error"
      ],
      "metadata": {
        "id": "RTfkShIXuHYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScratchLinearRegression():\n",
        "    \"\"\"\n",
        "    Scratch implementation of linear regression\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    num_iter : int\n",
        "      number of iterations\n",
        "    lr : float      \n",
        "      learning rate\n",
        "    no_bias : bool\n",
        "      If we do not include the bias term True\n",
        "    verbose : bool\n",
        "      When outputting the learning process True\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    self.coef_ : ndarray, shape (n_features,)\n",
        "      parameter\n",
        "    self.loss : ndarray, shape (self.iter,)\n",
        "      Record loss on training data\n",
        "    self.val_loss : ndarray, shape (self.iter,)\n",
        "      Record Loss Against Validation Data\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
        "        # Record hyperparameters as attributes\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.no_bias = no_bias\n",
        "        self.verbose = verbose\n",
        "        # Prepare an array to record losses\n",
        "        self.loss = np.zeros(self.iter)\n",
        "        self.val_loss = np.zeros(self.iter)\n",
        "\n",
        "    def _linear_hypothesis(self, X):\n",
        "        \"\"\"   \n",
        "        compute the linear hypothesis function\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "          training data\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ndarray, shape (n_samples, 1)\n",
        "        Estimation result by linear hypothesis function\n",
        "\n",
        "        \"\"\"\n",
        "        pred = X @ self.theta\n",
        "        return pred\n",
        "\n",
        "    def _gradient_descent(self, X, y):\n",
        "      \"\"\"\n",
        "      gradient descent algorithm, update formula\n",
        "      \"\"\"\n",
        "      m = X.shape[0]\n",
        "      n = X.shape[1]\n",
        "      pred = self._linear_hypothesis(X)\n",
        "      for j in range(n):\n",
        "          gradient = 0\n",
        "          for i in range(m):\n",
        "              gradient += (pred[i] - y[i]) * X[i, j]\n",
        "          self.theta[j] = self.theta[j] - self.lr * (gradient / m)    \n",
        "        \n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        Learn linear regression. If validation data is input, the loss and accuracy for it are also calculated for each iteration.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            Features of training data\n",
        "        y : ndarray, shape (n_samples, )\n",
        "            Correct value of training data\n",
        "        X_val : ndarray, shape (n_samples, n_features)\n",
        "            Features of validation data\n",
        "        y_val : ndarray, shape (n_samples, )\n",
        "            Correct value of validation data\n",
        "        \"\"\"\n",
        "        if self.verbose:\n",
        "            #Output learning process when verbose is set to True\n",
        "            print()\n",
        "        pass\n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate using linear regression。\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            sample\n",
        "        Returns\n",
        "        -------\n",
        "            ndarray, shape (n_samples, 1)\n",
        "            Estimation result by linear regression\n",
        "        \"\"\"\n",
        "        if self.bias == True:\n",
        "            bias = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
        "            X = np.hstack([bias, X])\n",
        "        pred_y = self._linear_hypothesis(X)\n",
        "        return pred_y\n",
        "\n",
        "    def MSE(y_pred, y):\n",
        "        \"\"\"\n",
        "            Calculate Mean Squared Error\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            y_pred : ndarray, shape (n_samples,)\n",
        "              estimated value\n",
        "            y : 次の形のndarray, shape (n_samples,)\n",
        "              correct answer\n",
        "\n",
        "            Returns\n",
        "            ----------\n",
        "            mse : numpy.float\n",
        "              mean squared error\n",
        "            \"\"\"\n",
        "        mse = ((y_pred - y) ** 2).sum() / X.shape[0]\n",
        "        return mse"
      ],
      "metadata": {
        "id": "scmjINLDuIyH"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Question 5] Objective function"
      ],
      "metadata": {
        "id": "TsZw-XbXvMTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScratchLinearRegression():\n",
        "    \"\"\"\n",
        "    Scratch implementation of linear regression\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    num_iter : int\n",
        "      number of iterations\n",
        "    lr : float      \n",
        "      learning rate\n",
        "    no_bias : bool\n",
        "      If we do not include the bias term True\n",
        "    verbose : bool\n",
        "      When outputting the learning process True\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    self.coef_ : ndarray, shape (n_features,)\n",
        "      parameter\n",
        "    self.loss : ndarray, shape (self.iter,)\n",
        "      Record loss on training data\n",
        "    self.val_loss : ndarray, shape (self.iter,)\n",
        "      Record Loss Against Validation Data\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
        "        # Record hyperparameters as attributes\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.no_bias = no_bias\n",
        "        self.verbose = verbose\n",
        "        # Prepare an array to record losses\n",
        "        self.loss = np.zeros(self.iter)\n",
        "        self.val_loss = np.zeros(self.iter)\n",
        "\n",
        "    def _linear_hypothesis(self, X):\n",
        "        \"\"\"   \n",
        "        compute the linear hypothesis function\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "          training data\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ndarray, shape (n_samples, 1)\n",
        "        Estimation result by linear hypothesis function\n",
        "\n",
        "        \"\"\"\n",
        "        pred = X @ self.theta\n",
        "        return pred\n",
        "\n",
        "    def _gradient_descent(self, X, y):\n",
        "      \"\"\"\n",
        "      gradient descent algorithm, update formula\n",
        "      \"\"\"\n",
        "      m = X.shape[0]\n",
        "      n = X.shape[1]\n",
        "      pred = self._linear_hypothesis(X)\n",
        "      for j in range(n):\n",
        "          gradient = 0\n",
        "          for i in range(m):\n",
        "              gradient += (pred[i] - y[i]) * X[i, j]\n",
        "          self.theta[j] = self.theta[j] - self.lr * (gradient / m)    \n",
        "        \n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        Learn linear regression. If validation data is input, the loss and accuracy for it are also calculated for each iteration.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            Features of training data\n",
        "        y : ndarray, shape (n_samples, )\n",
        "            Correct value of training data\n",
        "        X_val : ndarray, shape (n_samples, n_features)\n",
        "            Features of validation data\n",
        "        y_val : ndarray, shape (n_samples, )\n",
        "            Correct value of validation data\n",
        "        \"\"\"\n",
        "        if self.verbose:\n",
        "            #Output learning process when verbose is set to True\n",
        "            print()\n",
        "        pass\n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate using linear regression。\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            sample\n",
        "        Returns\n",
        "        -------\n",
        "            ndarray, shape (n_samples, 1)\n",
        "            Estimation result by linear regression\n",
        "        \"\"\"\n",
        "        if self.bias == True:\n",
        "            bias = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
        "            X = np.hstack([bias, X])\n",
        "        pred_y = self._linear_hypothesis(X)\n",
        "        return pred_y\n",
        "\n",
        "    def MSE(y_pred, y):\n",
        "        \"\"\"\n",
        "            Calculate Mean Squared Error\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            y_pred : ndarray, shape (n_samples,)\n",
        "              estimated value\n",
        "            y : 次の形のndarray, shape (n_samples,)\n",
        "              correct answer\n",
        "\n",
        "            Returns\n",
        "            ----------\n",
        "            mse : numpy.float\n",
        "              mean squared error\n",
        "            \"\"\"\n",
        "        mse = ((y_pred - y) ** 2).sum() / X.shape[0]\n",
        "        return mse\n",
        "\n",
        "    def _loss_func(self,y_pred, y):\n",
        "        loss = self.MSE(pred, y)/2\n",
        "        return loss"
      ],
      "metadata": {
        "id": "xzF9PGULvQ-y"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Question 6] Learning and Estimation"
      ],
      "metadata": {
        "id": "TapFPhUgvnxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScratchLinearRegression():\n",
        "    \"\"\"\n",
        "    Scratch implementation of linear regression\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    num_iter : int\n",
        "      number of iterations\n",
        "    lr : float      \n",
        "      learning rate\n",
        "    no_bias : bool\n",
        "      If we do not include the bias term True\n",
        "    verbose : bool\n",
        "      When outputting the learning process True\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    self.coef_ : ndarray, shape (n_features,)\n",
        "      parameter\n",
        "    self.loss : ndarray, shape (self.iter,)\n",
        "      Record loss on training data\n",
        "    self.val_loss : ndarray, shape (self.iter,)\n",
        "      Record Loss Against Validation Data\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
        "        # Record hyperparameters as attributes\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.no_bias = no_bias\n",
        "        self.verbose = verbose\n",
        "        # Prepare an array to record losses\n",
        "        #self.loss = np.zeros(self.iter)\n",
        "        #self.val_loss = np.zeros(self.iter)\n",
        "        self.theta = np.array([])\n",
        "        self.loss = np.array([])\n",
        "        self.val_loss = np.array([])\n",
        "\n",
        "    def _linear_hypothesis(self, X):\n",
        "        \"\"\"   \n",
        "        compute the linear hypothesis function\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "          training data\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ndarray, shape (n_samples, 1)\n",
        "        Estimation result by linear hypothesis function\n",
        "\n",
        "        \"\"\"\n",
        "        y_pred = X @ self.theta\n",
        "        return y_pred\n",
        "\n",
        "    def _gradient_descent(self, X, y):\n",
        "      \"\"\"\n",
        "      gradient descent algorithm, update formula\n",
        "      \"\"\"\n",
        "      m = X.shape[0]\n",
        "      n = X.shape[1]\n",
        "      y_pred = self._linear_hypothesis(X)\n",
        "      for j in range(n):\n",
        "          gradient = 0\n",
        "          for i in range(m):\n",
        "              gradient += (y_pred[i] - y[i]) * X[i, j]\n",
        "          self.theta[j] = self.theta[j] - self.lr * (gradient / m)   \n",
        "\n",
        "    def MSE(self, y_pred, y):\n",
        "        \"\"\"\n",
        "            Calculate Mean Squared Error\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            y_pred : ndarray, shape (n_samples,)\n",
        "              estimated value\n",
        "            y : 次の形のndarray, shape (n_samples,)\n",
        "              correct answer\n",
        "\n",
        "            Returns\n",
        "            ----------\n",
        "            mse : numpy.float\n",
        "              mean squared error\n",
        "            \"\"\"\n",
        "        mse = ((y_pred - y) ** 2).sum() / X.shape[0]\n",
        "        return mse\n",
        "\n",
        "    def _loss_func(self, y_pred, y):\n",
        "        loss = self.MSE(y_pred, y)/2\n",
        "        return loss \n",
        "        \n",
        "    def fit(self, X, y, X_val, y_val):\n",
        "        \"\"\"\n",
        "        Learn linear regression. If validation data is input, the loss and accuracy for it are also calculated for each iteration.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            Features of training data\n",
        "        y : ndarray, shape (n_samples, )\n",
        "            Correct value of training data\n",
        "        X_val : ndarray, shape (n_samples, n_features)\n",
        "            Features of validation data\n",
        "        y_val : ndarray, shape (n_samples, )\n",
        "            Correct value of validation data\n",
        "        \"\"\"        \n",
        "        if self.no_bias == True:\n",
        "            no_bias = np.ones((X.shape[0], 1))\n",
        "            X = np.hstack((no_bias, X))\n",
        "            no_bias = np.ones((X_val.shape[0], 1))\n",
        "            X_val = np.hstack((no_bias, X_val))\n",
        "        self.theta = np.zeros(X.shape[1])\n",
        "        self.theta = self.theta.reshape(X.shape[1], 1)\n",
        "        for i in range(self.iter):\n",
        "            pred = self._linear_hypothesis(X)\n",
        "            pred_val = self._linear_hypothesis(X_val)\n",
        "            self._gradient_descent(X, y)\n",
        "            loss = self._loss_func(pred, y)\n",
        "            self.loss = np.append(self.loss, loss)\n",
        "            loss_val = self._loss_func(pred_val, y_val)\n",
        "            self.val_loss = np.append(self.val_loss, loss_val)\n",
        "            if self.verbose:\n",
        "                #Output learning process when verbose is set to True\n",
        "                print('The {}th training loss is{}'.format(i,loss))    \n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate using linear regression。\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            sample\n",
        "        Returns\n",
        "        -------\n",
        "            ndarray, shape (n_samples, 1)\n",
        "            Estimation result by linear regression\n",
        "        \"\"\"\n",
        "        if self.bias == True:\n",
        "            bias = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
        "            X = np.hstack([bias, X])\n",
        "        pred_y = self._linear_hypothesis(X)\n",
        "        return pred_y\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "Y1qcCXdXv7fw"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "dataset = pd.read_csv(\"train.csv\")\n",
        "X = dataset.loc[:, ['GrLivArea', 'YearBuilt']]\n",
        "y = dataset.loc[:, ['SalePrice']]\n",
        "X = X.values\n",
        "y = y.values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.fit_transform(X_test)"
      ],
      "metadata": {
        "id": "UjDh3aZNwi6m"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "slr = ScratchLinearRegression(num_iter=100, lr=0.01, no_bias=True, verbose=True)\n",
        "slr.fit(X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEfB1FOewmpd",
        "outputId": "700f2073-1348-4a36-9150-ccdaf3399b2d"
      },
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 0th training loss is15309686354.840069\n",
            "The 1th training loss is15016427837.371342\n",
            "The 2th training loss is14729132672.02636\n",
            "The 3th training loss is14447679167.526972\n",
            "The 4th training loss is14171948125.645264\n",
            "The 5th training loss is13901822789.907007\n",
            "The 6th training loss is13637188795.355642\n",
            "The 7th training loss is13377934119.35473\n",
            "The 8th training loss is13123949033.407312\n",
            "The 9th training loss is12875126055.971035\n",
            "The 10th training loss is12631359906.248367\n",
            "The 11th training loss is12392547458.931652\n",
            "The 12th training loss is12158587699.883133\n",
            "The 13th training loss is11929381682.730608\n",
            "The 14th training loss is11704832486.35959\n",
            "The 15th training loss is11484845173.283485\n",
            "The 16th training loss is11269326748.87345\n",
            "The 17th training loss is11058186121.43016\n",
            "The 18th training loss is10851334063.079977\n",
            "The 19th training loss is10648683171.47842\n",
            "The 20th training loss is10450147832.304169\n",
            "The 21th training loss is10255644182.527239\n",
            "The 22th training loss is10065090074.435228\n",
            "The 23th training loss is9878405040.401924\n",
            "The 24th training loss is9695510258.382874\n",
            "The 25th training loss is9516328518.122856\n",
            "The 26th training loss is9340784188.060467\n",
            "The 27th training loss is9168803182.915375\n",
            "The 28th training loss is9000312931.944115\n",
            "The 29th training loss is8835242347.850521\n",
            "The 30th training loss is8673521796.337286\n",
            "The 31th training loss is8515083066.28529\n",
            "The 32th training loss is8359859340.547756\n",
            "The 33th training loss is8207785167.346468\n",
            "The 34th training loss is8058796432.257553\n",
            "The 35th training loss is7912830330.774667\n",
            "The 36th training loss is7769825341.437583\n",
            "The 37th training loss is7629721199.514485\n",
            "The 38th training loss is7492458871.226515\n",
            "The 39th training loss is7357980528.503322\n",
            "The 40th training loss is7226229524.258642\n",
            "The 41th training loss is7097150368.175131\n",
            "The 42th training loss is6970688702.987913\n",
            "The 43th training loss is6846791281.256523\n",
            "The 44th training loss is6725405942.615145\n",
            "The 45th training loss is6606481591.491232\n",
            "The 46th training loss is6489968175.282834\n",
            "The 47th training loss is6375816662.985138\n",
            "The 48th training loss is6263979024.256938\n",
            "The 49th training loss is6154408208.917907\n",
            "The 50th training loss is6047058126.867805\n",
            "The 51th training loss is5941883628.4188595\n",
            "The 52th training loss is5838840485.032814\n",
            "The 53th training loss is5737885370.454237\n",
            "The 54th training loss is5638975842.231935\n",
            "The 55th training loss is5542070323.62043\n",
            "The 56th training loss is5447128085.853662\n",
            "The 57th training loss is5354109230.783196\n",
            "The 58th training loss is5262974673.873447\n",
            "The 59th training loss is5173686127.546501\n",
            "The 60th training loss is5086206084.869344\n",
            "The 61th training loss is5000497803.57641\n",
            "The 62th training loss is4916525290.420528\n",
            "The 63th training loss is4834253285.845491\n",
            "The 64th training loss is4753647248.973589\n",
            "The 65th training loss is4674673342.9016285\n",
            "The 66th training loss is4597298420.299048\n",
            "The 67th training loss is4521490009.301919\n",
            "The 68th training loss is4447216299.69668\n",
            "The 69th training loss is4374446129.387689\n",
            "The 70th training loss is4303148971.142675\n",
            "The 71th training loss is4233294919.6104097\n",
            "The 72th training loss is4164854678.604927\n",
            "The 73th training loss is4097799548.650848\n",
            "The 74th training loss is4032101414.784387\n",
            "The 75th training loss is3967732734.6047797\n",
            "The 76th training loss is3904666526.570969\n",
            "The 77th training loss is3842876358.5384965\n",
            "The 78th training loss is3782336336.5316257\n",
            "The 79th training loss is3723021093.7458825\n",
            "The 80th training loss is3664905779.776231\n",
            "The 81th training loss is3607966050.0662518\n",
            "The 82th training loss is3552178055.5737653\n",
            "The 83th training loss is3497518432.6484323\n",
            "The 84th training loss is3443964293.1169815\n",
            "The 85th training loss is3391493214.571761\n",
            "The 86th training loss is3340083230.8584538\n",
            "The 87th training loss is3289712822.7588277\n",
            "The 88th training loss is3240360908.8645267\n",
            "The 89th training loss is3192006836.6379495\n",
            "The 90th training loss is3144630373.6563797\n",
            "The 91th training loss is3098211699.035579\n",
            "The 92th training loss is3052731395.0291686\n",
            "The 93th training loss is3008170438.8001485\n",
            "The 94th training loss is2964510194.361053\n",
            "The 95th training loss is2921732404.6792345\n",
            "The 96th training loss is2879819183.9439\n",
            "The 97th training loss is2838753009.991562\n",
            "The 98th training loss is2798516716.8866463\n",
            "The 99th training loss is2759093487.6540694\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Question 7] Learning curve plot"
      ],
      "metadata": {
        "id": "FmGP0V-M1No9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.plot(slr.loss)\n",
        "plt.plot(slr.val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "dhHVBvVE09ts",
        "outputId": "85c22a94-c76b-4c16-b131-c95a17b9709b"
      },
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f5f19f41340>]"
            ]
          },
          "metadata": {},
          "execution_count": 211
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}