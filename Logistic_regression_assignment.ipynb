{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPKxnqIeEhgHOXWr/+wsOxn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dashnyam7/DIC_assignment/blob/main/Logistic_regression_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Problem 1] Hypothetical function"
      ],
      "metadata": {
        "id": "GGhR6JEDgI7D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2vR-TRIdgF4C"
      },
      "outputs": [],
      "source": [
        "def _sigmoid(self,y):\n",
        "        \"\"\"sigmoid function\"\"\"\n",
        "        return 1 / (1 + np.exp(-y))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _logistic_hypothesis(self, X):\n",
        "    pred = X @ self.theta\n",
        "    pred = self._sigmoid(pred)\n",
        "    return pred"
      ],
      "metadata": {
        "id": "e9bs1MkAg6SV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Question 2] The steepest descent method"
      ],
      "metadata": {
        "id": "l-bdTKe6gqUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _gradient_descent(self, X, y):\n",
        "    m = X.shape[0]\n",
        "    n = X.shape[1]\n",
        "    pred = self._logistic_hypothesis(X)\n",
        "    for j in range(n):\n",
        "        gradient = 0\n",
        "        for i in range(m):\n",
        "            gradient += (pred[i] - y[i]) * X[i, j]\n",
        "        self.theta[j] = self.theta[j] - self.lr * ((gradient+self.lam*self.theta[j]) / m)"
      ],
      "metadata": {
        "id": "8OtxWjoMgtv-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Question 3] Presumption"
      ],
      "metadata": {
        "id": "Q3pcxn_jg_sl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate labels using logistic regression。\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            sample\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "            ndarray, shape (n_samples, 1)\n",
        "            Estimation result by logistic regression\n",
        "        \"\"\"\n",
        "\n",
        "        if self.bias == True:\n",
        "            a = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
        "            X = np.hstack([a, X])\n",
        "        return  np.where(self._logistic_hypothesis(X) >= 0.5,1,0)\n",
        "\n",
        "def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Estimate probabilities using logistic regression。\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            sample\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "            ndarray, shape (n_samples, 1)\n",
        "            Estimation result by logistic regression\n",
        "        \"\"\"\n",
        "\n",
        "        if self.bias == True:\n",
        "            a = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
        "            X = np.hstack([a, X])\n",
        "        pred = self._logistic_hypothesis(X)\n",
        "        return pred"
      ],
      "metadata": {
        "id": "tDGHN5cIhEQN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Question 4] Objective function"
      ],
      "metadata": {
        "id": "nAT_VImih0we"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _loss_func(self, pred, y):\n",
        "    error = 0\n",
        "    for i in range(y.shape[0]):\n",
        "        error += -np.sum(y[i] *  np.log(pred[i])+(1-y[i]) *  np.log(1-pred[i]))\n",
        "    loss = error / (y.shape[0])\n",
        "    loss = loss + np.sum(self.theta**2)*self.lam/(2 * y.shape[0])\n",
        "    return loss"
      ],
      "metadata": {
        "id": "gdalimTAh4sf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Question 5] Learning and Estimation"
      ],
      "metadata": {
        "id": "vWK2TrDLh4R9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScratchLogisticRegression():\n",
        "    def __init__(self, num_iter, lr, bias, verbose,lam):\n",
        "        self.num_iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.bias = bias\n",
        "        self.verbose = verbose\n",
        "        self.lam = lam\n",
        "        self.theta = np.array([])\n",
        "        self.loss = np.array([])\n",
        "        self.val_loss = np.array([])\n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        if self.bias == True:\n",
        "            bias = np.ones((X.shape[0], 1))\n",
        "            X = np.hstack((bias, X))\n",
        "            bias = np.ones((X_val.shape[0], 1))\n",
        "            X_val = np.hstack((bias, X_val))\n",
        "        self.theta = np.zeros(X.shape[1])\n",
        "        self.theta = self.theta.reshape(X.shape[1], 1)\n",
        "        for i in range(self.num_iter):\n",
        "            pred = self._logistic_hypothesis(X)\n",
        "            pred_val = self._logistic_hypothesis(X_val)\n",
        "            self._gradient_descent(X, y)\n",
        "            loss = self._loss_func(pred, y)\n",
        "            self.loss = np.append(self.loss, loss)\n",
        "            loss_val = self._loss_func(pred_val, y_val)\n",
        "            self.val_loss = np.append(self.val_loss, loss_val)\n",
        "            if self.verbose == True:\n",
        "                print('{}th training loss is {}'.format(i,loss))\n",
        "\n",
        "    def _gradient_descent(self, X, y):\n",
        "        m = X.shape[0]\n",
        "        n = X.shape[1]\n",
        "        pred = self._logistic_hypothesis(X)\n",
        "        for j in range(n):\n",
        "            gradient = 0\n",
        "            for i in range(m):\n",
        "                gradient += (pred[i] - y[i]) * X[i, j]\n",
        "            self.theta[j] = self.theta[j] - self.lr * ((gradient+self.lam*self.theta[j]) / m)\n",
        "     \n",
        "    def _sigmoid(self,y):\n",
        "        \"\"\"sigmoid function\"\"\"\n",
        "        return 1 / (1 + np.exp(-y))\n",
        "        \n",
        "    def _logistic_hypothesis(self, X):\n",
        "        pred = X @ self.theta\n",
        "        pred = self._sigmoid(pred)\n",
        "        return pred\n",
        "       \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate labels using logistic regression。\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            sample\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "            ndarray, shape (n_samples, 1)\n",
        "            Estimation result by logistic regression\n",
        "        \"\"\"\n",
        "\n",
        "        if self.bias == True:\n",
        "            a = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
        "            X = np.hstack([a, X])\n",
        "        return  np.where(self._logistic_hypothesis(X) >= 0.5,1,0)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Estimate probabilities using logistic regression。\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            sample\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "            ndarray, shape (n_samples, 1)\n",
        "            Estimation result by logistic regression\n",
        "        \"\"\"\n",
        "\n",
        "        if self.bias == True:\n",
        "            a = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
        "            X = np.hstack([a, X])\n",
        "        pred = self._logistic_hypothesis(X)\n",
        "        return pred\n",
        "        \n",
        "    def _loss_func(self, pred, y):\n",
        "        error = 0\n",
        "        for i in range(y.shape[0]):\n",
        "            error += -np.sum(y[i] *  np.log(pred[i])+(1-y[i]) *  np.log(1-pred[i]))\n",
        "        loss = error / (y.shape[0])\n",
        "        loss = loss + np.sum(self.theta**2)*self.lam/(2 * y.shape[0])\n",
        "        return loss"
      ],
      "metadata": {
        "id": "cm7yoNVhiDQG"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "iris = load_iris()\n",
        "X = iris.data[:100,:]\n",
        "y = iris.target[:100]\n",
        "(X_train, X_test, y_train, y_test) = train_test_split(X, y, test_size=0.2)"
      ],
      "metadata": {
        "id": "CUveKQoWnQKf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "slr = ScratchLogisticRegression(num_iter=100, lr=0.01, bias=True, verbose=True,lam=0.1)\n",
        "slr.fit(X_train, y_train, X_test, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0nrrXmLnTIM",
        "outputId": "88de4404-c1f0-4407-9da5-7aecd198e987"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0th training loss is 0.6931472133128265\n",
            "1th training loss is 0.6879536700056779\n",
            "2th training loss is 0.682930008189488\n",
            "3th training loss is 0.678046351847922\n",
            "4th training loss is 0.6732797587115128\n",
            "5th training loss is 0.6686125974688274\n",
            "6th training loss is 0.6640313005665729\n",
            "7th training loss is 0.6595254076591273\n",
            "8th training loss is 0.6550868330792795\n",
            "9th training loss is 0.6507093054739871\n",
            "10th training loss is 0.6463879394735672\n",
            "11th training loss is 0.6421189084633259\n",
            "12th training loss is 0.637899194688378\n",
            "13th training loss is 0.6337263984649925\n",
            "14th training loss is 0.6295985925433228\n",
            "15th training loss is 0.6255142109485502\n",
            "16th training loss is 0.6214719641439347\n",
            "17th training loss is 0.6174707742856993\n",
            "18th training loss is 0.6135097258128288\n",
            "19th training loss is 0.6095880277405088\n",
            "20th training loss is 0.6057049848856065\n",
            "21th training loss is 0.6018599759089209\n",
            "22th training loss is 0.5980524365598813\n",
            "23th training loss is 0.594281846891694\n",
            "24th training loss is 0.5905477215066983\n",
            "25th training loss is 0.5868496021143372\n",
            "26th training loss is 0.5831870518540481\n",
            "27th training loss is 0.5795596509650435\n",
            "28th training loss is 0.575966993483899\n",
            "29th training loss is 0.572408684726391\n",
            "30th training loss is 0.5688843393676596\n",
            "31th training loss is 0.5653935799787815\n",
            "32th training loss is 0.561936035911404\n",
            "33th training loss is 0.5585113424477467\n",
            "34th training loss is 0.5551191401528283\n",
            "35th training loss is 0.5517590743807365\n",
            "36th training loss is 0.5484307948981546\n",
            "37th training loss is 0.5451339555970688\n",
            "38th training loss is 0.5418682142752357\n",
            "39th training loss is 0.5386332324680618\n",
            "40th training loss is 0.5354286753194263\n",
            "41th training loss is 0.5322542114819385\n",
            "42th training loss is 0.52910951303938\n",
            "43th training loss is 0.5259942554458101\n",
            "44th training loss is 0.5229081174771317\n",
            "45th training loss is 0.5198507811919109\n",
            "46th training loss is 0.5168219318990322\n",
            "47th training loss is 0.513821258130329\n",
            "48th training loss is 0.5108484516168075\n",
            "49th training loss is 0.5079032072673957\n",
            "50th training loss is 0.5049852231494284\n",
            "51th training loss is 0.5020942004702688\n",
            "52th training loss is 0.49922984355961764\n",
            "53th training loss is 0.4963918598521864\n",
            "54th training loss is 0.4935799598704825\n",
            "55th training loss is 0.49079385720753393\n",
            "56th training loss is 0.4880332685094258\n",
            "57th training loss is 0.4852979134575638\n",
            "58th training loss is 0.48258751475059836\n",
            "59th training loss is 0.4799017980859775\n",
            "60th training loss is 0.47724049214110703\n",
            "61th training loss is 0.4746033285541056\n",
            "62th training loss is 0.4719900419041582\n",
            "63th training loss is 0.46940036969147264\n",
            "64th training loss is 0.4668340523168515\n",
            "65th training loss is 0.464290833060893\n",
            "66th training loss is 0.4617704580628411\n",
            "67th training loss is 0.4592726762991008\n",
            "68th training loss is 0.4567972395614398\n",
            "69th training loss is 0.4543439024348962\n",
            "70th training loss is 0.4519124222754135\n",
            "71th training loss is 0.4495025591872245\n",
            "72th training loss is 0.4471140760000035\n",
            "73th training loss is 0.4447467382458039\n",
            "74th training loss is 0.4424003141358071\n",
            "75th training loss is 0.44007457453689736\n",
            "76th training loss is 0.43776929294807987\n",
            "77th training loss is 0.43548424547676623\n",
            "78th training loss is 0.43321921081493614\n",
            "79th training loss is 0.4309739702151992\n",
            "80th training loss is 0.42874830746676956\n",
            "81th training loss is 0.42654200887136723\n",
            "82th training loss is 0.42435486321906773\n",
            "83th training loss is 0.42218666176410313\n",
            "84th training loss is 0.4200371982006394\n",
            "85th training loss is 0.4179062686385363\n",
            "86th training loss is 0.41579367157910413\n",
            "87th training loss is 0.4136992078908705\n",
            "88th training loss is 0.41162268078536557\n",
            "89th training loss is 0.40956389579293984\n",
            "90th training loss is 0.40752266073862375\n",
            "91th training loss is 0.4054987857180374\n",
            "92th training loss is 0.40349208307336315\n",
            "93th training loss is 0.4015023673693863\n",
            "94th training loss is 0.3995294553696156\n",
            "95th training loss is 0.3975731660124892\n",
            "96th training loss is 0.39563332038767723\n",
            "97th training loss is 0.3937097417124835\n",
            "98th training loss is 0.39180225530835666\n",
            "99th training loss is 0.3899106885775181\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "slr.predict(X_test).T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrcOLaZKop8I",
        "outputId": "54853177-abb0-44f9-e870-753dd05aa280"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8nV2iLHqTmB",
        "outputId": "8f8544e9-75ce-4274-b190-75fd53ec8f86"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Question 6] Learning curve plot"
      ],
      "metadata": {
        "id": "_kCIwNH5iGmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.plot(slr.loss)\n",
        "plt.plot(slr.val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "WmD4CjaciPie",
        "outputId": "162a1391-3edf-4a8f-86a0-b8dd7be1db12"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fed187bda30>]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Problem 7] Visualization of the decision region"
      ],
      "metadata": {
        "id": "8UVY0VaSiJt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = iris.data[:100,:2]\n",
        "y = iris.target[:100]\n",
        "(X_train, X_test, y_train, y_test) = train_test_split(X, y, test_size=0.2)\n",
        "slr = ScratchLogisticRegression(num_iter=2000, lr=0.001,bias=True,verbose=True,lam = 0.1)\n",
        "slr.fit(X_train, y_train,X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Owr9Fs1s0Qr",
        "outputId": "1bcb5860-fba2-49d5-cc33-d97e8559f293"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0th training loss is 0.693147180611118\n",
            "1th training loss is 0.6930653778095691\n",
            "2th training loss is 0.6929838636520891\n",
            "3th training loss is 0.6929026324793343\n",
            "4th training loss is 0.692821678747402\n",
            "5th training loss is 0.6927409970254876\n",
            "6th training loss is 0.6926605819935756\n",
            "7th training loss is 0.6925804284401867\n",
            "8th training loss is 0.692500531260169\n",
            "9th training loss is 0.6924208854525274\n",
            "10th training loss is 0.6923414861183076\n",
            "11th training loss is 0.6922623284585133\n",
            "12th training loss is 0.6921834077720715\n",
            "13th training loss is 0.6921047194538371\n",
            "14th training loss is 0.692026258992637\n",
            "15th training loss is 0.6919480219693572\n",
            "16th training loss is 0.6918700040550635\n",
            "17th training loss is 0.6917922010091658\n",
            "18th training loss is 0.6917146086776146\n",
            "19th training loss is 0.6916372229911402\n",
            "20th training loss is 0.6915600399635208\n",
            "21th training loss is 0.6914830556898905\n",
            "22th training loss is 0.6914062663450802\n",
            "23th training loss is 0.6913296681819915\n",
            "24th training loss is 0.6912532575300057\n",
            "25th training loss is 0.6911770307934229\n",
            "26th training loss is 0.6911009844499338\n",
            "27th training loss is 0.6910251150491233\n",
            "28th training loss is 0.6909494192110025\n",
            "29th training loss is 0.6908738936245726\n",
            "30th training loss is 0.6907985350464175\n",
            "31th training loss is 0.6907233402993238\n",
            "32th training loss is 0.690648306270931\n",
            "33th training loss is 0.6905734299124062\n",
            "34th training loss is 0.690498708237151\n",
            "35th training loss is 0.6904241383195279\n",
            "36th training loss is 0.690349717293616\n",
            "37th training loss is 0.6902754423519959\n",
            "38th training loss is 0.690201310744551\n",
            "39th training loss is 0.6901273197773001\n",
            "40th training loss is 0.6900534668112518\n",
            "41th training loss is 0.6899797492612803\n",
            "42th training loss is 0.6899061645950271\n",
            "43th training loss is 0.6898327103318223\n",
            "44th training loss is 0.6897593840416298\n",
            "45th training loss is 0.6896861833440145\n",
            "46th training loss is 0.6896131059071274\n",
            "47th training loss is 0.689540149446716\n",
            "48th training loss is 0.6894673117251486\n",
            "49th training loss is 0.6893945905504669\n",
            "50th training loss is 0.6893219837754501\n",
            "51th training loss is 0.689249489296701\n",
            "52th training loss is 0.6891771050537536\n",
            "53th training loss is 0.6891048290281938\n",
            "54th training loss is 0.6890326592428014\n",
            "55th training loss is 0.6889605937607088\n",
            "56th training loss is 0.6888886306845767\n",
            "57th training loss is 0.6888167681557873\n",
            "58th training loss is 0.6887450043536517\n",
            "59th training loss is 0.6886733374946361\n",
            "60th training loss is 0.6886017658316048\n",
            "61th training loss is 0.6885302876530733\n",
            "62th training loss is 0.6884589012824808\n",
            "63th training loss is 0.6883876050774792\n",
            "64th training loss is 0.688316397429231\n",
            "65th training loss is 0.6882452767617276\n",
            "66th training loss is 0.6881742415311156\n",
            "67th training loss is 0.6881032902250428\n",
            "68th training loss is 0.6880324213620123\n",
            "69th training loss is 0.687961633490754\n",
            "70th training loss is 0.6878909251896057\n",
            "71th training loss is 0.6878202950659085\n",
            "72th training loss is 0.6877497417554153\n",
            "73th training loss is 0.687679263921708\n",
            "74th training loss is 0.6876088602556317\n",
            "75th training loss is 0.6875385294747347\n",
            "76th training loss is 0.687468270322727\n",
            "77th training loss is 0.6873980815689412\n",
            "78th training loss is 0.6873279620078117\n",
            "79th training loss is 0.687257910458361\n",
            "80th training loss is 0.6871879257636982\n",
            "81th training loss is 0.6871180067905251\n",
            "82th training loss is 0.6870481524286564\n",
            "83th training loss is 0.686978361590546\n",
            "84th training loss is 0.686908633210823\n",
            "85th training loss is 0.6868389662458432\n",
            "86th training loss is 0.6867693596732394\n",
            "87th training loss is 0.686699812491491\n",
            "88th training loss is 0.686630323719497\n",
            "89th training loss is 0.6865608923961567\n",
            "90th training loss is 0.6864915175799639\n",
            "91th training loss is 0.6864221983486047\n",
            "92th training loss is 0.6863529337985669\n",
            "93th training loss is 0.6862837230447532\n",
            "94th training loss is 0.6862145652201076\n",
            "95th training loss is 0.6861454594752463\n",
            "96th training loss is 0.6860764049780939\n",
            "97th training loss is 0.6860074009135345\n",
            "98th training loss is 0.6859384464830608\n",
            "99th training loss is 0.6858695409044377\n",
            "100th training loss is 0.6858006834113676\n",
            "101th training loss is 0.6857318732531655\n",
            "102th training loss is 0.6856631096944418\n",
            "103th training loss is 0.685594392014787\n",
            "104th training loss is 0.6855257195084666\n",
            "105th training loss is 0.6854570914841229\n",
            "106th training loss is 0.685388507264479\n",
            "107th training loss is 0.6853199661860512\n",
            "108th training loss is 0.6852514675988695\n",
            "109th training loss is 0.6851830108661984\n",
            "110th training loss is 0.6851145953642691\n",
            "111th training loss is 0.6850462204820132\n",
            "112th training loss is 0.6849778856208029\n",
            "113th training loss is 0.6849095901941977\n",
            "114th training loss is 0.6848413336276962\n",
            "115th training loss is 0.6847731153584886\n",
            "116th training loss is 0.6847049348352233\n",
            "117th training loss is 0.6846367915177669\n",
            "118th training loss is 0.6845686848769802\n",
            "119th training loss is 0.6845006143944891\n",
            "120th training loss is 0.6844325795624688\n",
            "121th training loss is 0.6843645798834256\n",
            "122th training loss is 0.6842966148699853\n",
            "123th training loss is 0.6842286840446893\n",
            "124th training loss is 0.6841607869397897\n",
            "125th training loss is 0.6840929230970517\n",
            "126th training loss is 0.6840250920675587\n",
            "127th training loss is 0.6839572934115236\n",
            "128th training loss is 0.683889526698099\n",
            "129th training loss is 0.6838217915051982\n",
            "130th training loss is 0.6837540874193151\n",
            "131th training loss is 0.6836864140353464\n",
            "132th training loss is 0.6836187709564236\n",
            "133th training loss is 0.6835511577937428\n",
            "134th training loss is 0.6834835741664002\n",
            "135th training loss is 0.683416019701231\n",
            "136th training loss is 0.6833484940326509\n",
            "137th training loss is 0.6832809968025014\n",
            "138th training loss is 0.6832135276598983\n",
            "139th training loss is 0.6831460862610833\n",
            "140th training loss is 0.683078672269277\n",
            "141th training loss is 0.6830112853545389\n",
            "142th training loss is 0.6829439251936255\n",
            "143th training loss is 0.6828765914698542\n",
            "144th training loss is 0.6828092838729689\n",
            "145th training loss is 0.6827420020990101\n",
            "146th training loss is 0.6826747458501852\n",
            "147th training loss is 0.6826075148347421\n",
            "148th training loss is 0.6825403087668457\n",
            "149th training loss is 0.6824731273664596\n",
            "150th training loss is 0.6824059703592247\n",
            "151th training loss is 0.6823388374763438\n",
            "152th training loss is 0.6822717284544695\n",
            "153th training loss is 0.6822046430355916\n",
            "154th training loss is 0.6821375809669273\n",
            "155th training loss is 0.6820705420008174\n",
            "156th training loss is 0.6820035258946183\n",
            "157th training loss is 0.6819365324106011\n",
            "158th training loss is 0.6818695613158522\n",
            "159th training loss is 0.6818026123821723\n",
            "160th training loss is 0.6817356853859833\n",
            "161th training loss is 0.6816687801082305\n",
            "162th training loss is 0.6816018963342936\n",
            "163th training loss is 0.6815350338538946\n",
            "164th training loss is 0.6814681924610068\n",
            "165th training loss is 0.6814013719537735\n",
            "166th training loss is 0.6813345721344171\n",
            "167th training loss is 0.6812677928091593\n",
            "168th training loss is 0.6812010337881378\n",
            "169th training loss is 0.6811342948853276\n",
            "170th training loss is 0.6810675759184622\n",
            "171th training loss is 0.6810008767089551\n",
            "172th training loss is 0.6809341970818282\n",
            "173th training loss is 0.6808675368656334\n",
            "174th training loss is 0.6808008958923859\n",
            "175th training loss is 0.680734273997489\n",
            "176th training loss is 0.680667671019667\n",
            "177th training loss is 0.680601086800898\n",
            "178th training loss is 0.6805345211863449\n",
            "179th training loss is 0.6804679740242928\n",
            "180th training loss is 0.6804014451660841\n",
            "181th training loss is 0.6803349344660571\n",
            "182th training loss is 0.6802684417814836\n",
            "183th training loss is 0.680201966972509\n",
            "184th training loss is 0.6801355099020945\n",
            "185th training loss is 0.6800690704359587\n",
            "186th training loss is 0.6800026484425234\n",
            "187th training loss is 0.6799362437928549\n",
            "188th training loss is 0.6798698563606128\n",
            "189th training loss is 0.6798034860219961\n",
            "190th training loss is 0.6797371326556915\n",
            "191th training loss is 0.6796707961428222\n",
            "192th training loss is 0.6796044763668985\n",
            "193th training loss is 0.6795381732137689\n",
            "194th training loss is 0.6794718865715728\n",
            "195th training loss is 0.6794056163306937\n",
            "196th training loss is 0.6793393623837121\n",
            "197th training loss is 0.679273124625362\n",
            "198th training loss is 0.6792069029524872\n",
            "199th training loss is 0.6791406972639967\n",
            "200th training loss is 0.6790745074608238\n",
            "201th training loss is 0.6790083334458843\n",
            "202th training loss is 0.6789421751240368\n",
            "203th training loss is 0.6788760324020408\n",
            "204th training loss is 0.6788099051885201\n",
            "205th training loss is 0.6787437933939253\n",
            "206th training loss is 0.6786776969304934\n",
            "207th training loss is 0.6786116157122145\n",
            "208th training loss is 0.6785455496547942\n",
            "209th training loss is 0.6784794986756185\n",
            "210th training loss is 0.6784134626937217\n",
            "211th training loss is 0.6783474416297495\n",
            "212th training loss is 0.6782814354059284\n",
            "213th training loss is 0.678215443946033\n",
            "214th training loss is 0.6781494671753535\n",
            "215th training loss is 0.6780835050206664\n",
            "216th training loss is 0.6780175574102016\n",
            "217th training loss is 0.6779516242736167\n",
            "218th training loss is 0.6778857055419628\n",
            "219th training loss is 0.677819801147661\n",
            "220th training loss is 0.6777539110244701\n",
            "221th training loss is 0.6776880351074629\n",
            "222th training loss is 0.6776221733329969\n",
            "223th training loss is 0.67755632563869\n",
            "224th training loss is 0.6774904919633928\n",
            "225th training loss is 0.6774246722471645\n",
            "226th training loss is 0.6773588664312489\n",
            "227th training loss is 0.677293074458049\n",
            "228th training loss is 0.6772272962711029\n",
            "229th training loss is 0.6771615318150629\n",
            "230th training loss is 0.6770957810356708\n",
            "231th training loss is 0.6770300438797364\n",
            "232th training loss is 0.6769643202951153\n",
            "233th training loss is 0.6768986102306878\n",
            "234th training loss is 0.6768329136363382\n",
            "235th training loss is 0.6767672304629337\n",
            "236th training loss is 0.6767015606623051\n",
            "237th training loss is 0.6766359041872274\n",
            "238th training loss is 0.676570260991398\n",
            "239th training loss is 0.6765046310294219\n",
            "240th training loss is 0.6764390142567897\n",
            "241th training loss is 0.6763734106298608\n",
            "242th training loss is 0.6763078201058463\n",
            "243th training loss is 0.676242242642791\n",
            "244th training loss is 0.6761766781995575\n",
            "245th training loss is 0.6761111267358049\n",
            "246th training loss is 0.6760455882119802\n",
            "247th training loss is 0.675980062589296\n",
            "248th training loss is 0.6759145498297171\n",
            "249th training loss is 0.6758490498959451\n",
            "250th training loss is 0.675783562751404\n",
            "251th training loss is 0.6757180883602244\n",
            "252th training loss is 0.6756526266872283\n",
            "253th training loss is 0.6755871776979174\n",
            "254th training loss is 0.6755217413584585\n",
            "255th training loss is 0.6754563176356673\n",
            "256th training loss is 0.6753909064969992\n",
            "257th training loss is 0.6753255079105331\n",
            "258th training loss is 0.6752601218449603\n",
            "259th training loss is 0.675194748269572\n",
            "260th training loss is 0.6751293871542462\n",
            "261th training loss is 0.6750640384694352\n",
            "262th training loss is 0.6749987021861565\n",
            "263th training loss is 0.6749333782759788\n",
            "264th training loss is 0.674868066711011\n",
            "265th training loss is 0.674802767463893\n",
            "266th training loss is 0.6747374805077823\n",
            "267th training loss is 0.6746722058163457\n",
            "268th training loss is 0.674606943363747\n",
            "269th training loss is 0.6745416931246391\n",
            "270th training loss is 0.6744764550741519\n",
            "271th training loss is 0.6744112291878823\n",
            "272th training loss is 0.6743460154418885\n",
            "273th training loss is 0.674280813812675\n",
            "274th training loss is 0.6742156242771881\n",
            "275th training loss is 0.674150446812804\n",
            "276th training loss is 0.6740852813973229\n",
            "277th training loss is 0.6740201280089558\n",
            "278th training loss is 0.6739549866263221\n",
            "279th training loss is 0.6738898572284354\n",
            "280th training loss is 0.6738247397946995\n",
            "281th training loss is 0.6737596343048998\n",
            "282th training loss is 0.6736945407391923\n",
            "283th training loss is 0.6736294590781013\n",
            "284th training loss is 0.673564389302507\n",
            "285th training loss is 0.6734993313936422\n",
            "286th training loss is 0.6734342853330809\n",
            "287th training loss is 0.6733692511027368\n",
            "288th training loss is 0.673304228684851\n",
            "289th training loss is 0.6732392180619879\n",
            "290th training loss is 0.6731742192170282\n",
            "291th training loss is 0.6731092321331642\n",
            "292th training loss is 0.6730442567938897\n",
            "293th training loss is 0.6729792931829967\n",
            "294th training loss is 0.6729143412845698\n",
            "295th training loss is 0.6728494010829762\n",
            "296th training loss is 0.6727844725628658\n",
            "297th training loss is 0.6727195557091594\n",
            "298th training loss is 0.6726546505070491\n",
            "299th training loss is 0.6725897569419884\n",
            "300th training loss is 0.6725248749996874\n",
            "301th training loss is 0.6724600046661094\n",
            "302th training loss is 0.6723951459274655\n",
            "303th training loss is 0.6723302987702073\n",
            "304th training loss is 0.6722654631810244\n",
            "305th training loss is 0.672200639146838\n",
            "306th training loss is 0.6721358266547982\n",
            "307th training loss is 0.6720710256922765\n",
            "308th training loss is 0.6720062362468633\n",
            "309th training loss is 0.671941458306363\n",
            "310th training loss is 0.67187669185879\n",
            "311th training loss is 0.6718119368923621\n",
            "312th training loss is 0.6717471933955005\n",
            "313th training loss is 0.6716824613568219\n",
            "314th training loss is 0.6716177407651371\n",
            "315th training loss is 0.671553031609444\n",
            "316th training loss is 0.6714883338789284\n",
            "317th training loss is 0.6714236475629541\n",
            "318th training loss is 0.6713589726510659\n",
            "319th training loss is 0.6712943091329804\n",
            "320th training loss is 0.6712296569985862\n",
            "321th training loss is 0.6711650162379376\n",
            "322th training loss is 0.6711003868412535\n",
            "323th training loss is 0.6710357687989129\n",
            "324th training loss is 0.6709711621014516\n",
            "325th training loss is 0.6709065667395594\n",
            "326th training loss is 0.6708419827040766\n",
            "327th training loss is 0.670777409985991\n",
            "328th training loss is 0.6707128485764356\n",
            "329th training loss is 0.6706482984666854\n",
            "330th training loss is 0.6705837596481525\n",
            "331th training loss is 0.6705192321123864\n",
            "332th training loss is 0.6704547158510696\n",
            "333th training loss is 0.6703902108560141\n",
            "334th training loss is 0.6703257171191606\n",
            "335th training loss is 0.6702612346325751\n",
            "336th training loss is 0.6701967633884461\n",
            "337th training loss is 0.6701323033790816\n",
            "338th training loss is 0.6700678545969081\n",
            "339th training loss is 0.6700034170344664\n",
            "340th training loss is 0.669938990684412\n",
            "341th training loss is 0.6698745755395094\n",
            "342th training loss is 0.6698101715926321\n",
            "343th training loss is 0.6697457788367595\n",
            "344th training loss is 0.6696813972649757\n",
            "345th training loss is 0.6696170268704662\n",
            "346th training loss is 0.6695526676465167\n",
            "347th training loss is 0.6694883195865096\n",
            "348th training loss is 0.6694239826839239\n",
            "349th training loss is 0.6693596569323336\n",
            "350th training loss is 0.6692953423254021\n",
            "351th training loss is 0.6692310388568848\n",
            "352th training loss is 0.6691667465206249\n",
            "353th training loss is 0.6691024653105521\n",
            "354th training loss is 0.6690381952206799\n",
            "355th training loss is 0.6689739362451055\n",
            "356th training loss is 0.6689096883780075\n",
            "357th training loss is 0.6688454516136442\n",
            "358th training loss is 0.6687812259463503\n",
            "359th training loss is 0.6687170113705393\n",
            "360th training loss is 0.6686528078806981\n",
            "361th training loss is 0.6685886154713873\n",
            "362th training loss is 0.6685244341372385\n",
            "363th training loss is 0.6684602638729549\n",
            "364th training loss is 0.6683961046733079\n",
            "365th training loss is 0.6683319565331365\n",
            "366th training loss is 0.6682678194473465\n",
            "367th training loss is 0.668203693410907\n",
            "368th training loss is 0.6681395784188521\n",
            "369th training loss is 0.6680754744662777\n",
            "370th training loss is 0.66801138154834\n",
            "371th training loss is 0.6679472996602552\n",
            "372th training loss is 0.6678832287972984\n",
            "373th training loss is 0.6678191689548018\n",
            "374th training loss is 0.6677551201281532\n",
            "375th training loss is 0.6676910823127956\n",
            "376th training loss is 0.6676270555042265\n",
            "377th training loss is 0.6675630396979956\n",
            "378th training loss is 0.6674990348897037\n",
            "379th training loss is 0.6674350410750034\n",
            "380th training loss is 0.667371058249596\n",
            "381th training loss is 0.6673070864092325\n",
            "382th training loss is 0.6672431255497101\n",
            "383th training loss is 0.6671791756668729\n",
            "384th training loss is 0.6671152367566122\n",
            "385th training loss is 0.6670513088148622\n",
            "386th training loss is 0.666987391837602\n",
            "387th training loss is 0.6669234858208535\n",
            "388th training loss is 0.6668595907606799\n",
            "389th training loss is 0.6667957066531862\n",
            "390th training loss is 0.6667318334945188\n",
            "391th training loss is 0.666667971280862\n",
            "392th training loss is 0.6666041200084399\n",
            "393th training loss is 0.6665402796735143\n",
            "394th training loss is 0.6664764502723838\n",
            "395th training loss is 0.6664126318013842\n",
            "396th training loss is 0.6663488242568867\n",
            "397th training loss is 0.666285027635297\n",
            "398th training loss is 0.6662212419330557\n",
            "399th training loss is 0.6661574671466367\n",
            "400th training loss is 0.6660937032725469\n",
            "401th training loss is 0.6660299503073259\n",
            "402th training loss is 0.6659662082475434\n",
            "403th training loss is 0.665902477089802\n",
            "404th training loss is 0.665838756830733\n",
            "405th training loss is 0.6657750474669979\n",
            "406th training loss is 0.665711348995288\n",
            "407th training loss is 0.6656476614123222\n",
            "408th training loss is 0.6655839847148473\n",
            "409th training loss is 0.6655203188996381\n",
            "410th training loss is 0.6654566639634959\n",
            "411th training loss is 0.665393019903248\n",
            "412th training loss is 0.6653293867157475\n",
            "413th training loss is 0.665265764397873\n",
            "414th training loss is 0.665202152946528\n",
            "415th training loss is 0.6651385523586393\n",
            "416th training loss is 0.665074962631158\n",
            "417th training loss is 0.6650113837610584\n",
            "418th training loss is 0.6649478157453379\n",
            "419th training loss is 0.6648842585810149\n",
            "420th training loss is 0.664820712265131\n",
            "421th training loss is 0.6647571767947491\n",
            "422th training loss is 0.6646936521669518\n",
            "423th training loss is 0.6646301383788439\n",
            "424th training loss is 0.6645666354275485\n",
            "425th training loss is 0.6645031433102102\n",
            "426th training loss is 0.6644396620239926\n",
            "427th training loss is 0.6643761915660762\n",
            "428th training loss is 0.6643127319336639\n",
            "429th training loss is 0.6642492831239724\n",
            "430th training loss is 0.6641858451342394\n",
            "431th training loss is 0.6641224179617183\n",
            "432th training loss is 0.6640590016036808\n",
            "433th training loss is 0.6639955960574135\n",
            "434th training loss is 0.6639322013202227\n",
            "435th training loss is 0.6638688173894269\n",
            "436th training loss is 0.6638054442623628\n",
            "437th training loss is 0.6637420819363822\n",
            "438th training loss is 0.6636787304088512\n",
            "439th training loss is 0.6636153896771521\n",
            "440th training loss is 0.6635520597386798\n",
            "441th training loss is 0.6634887405908451\n",
            "442th training loss is 0.6634254322310718\n",
            "443th training loss is 0.6633621346567973\n",
            "444th training loss is 0.6632988478654741\n",
            "445th training loss is 0.6632355718545649\n",
            "446th training loss is 0.6631723066215472\n",
            "447th training loss is 0.6631090521639119\n",
            "448th training loss is 0.6630458084791597\n",
            "449th training loss is 0.6629825755648042\n",
            "450th training loss is 0.6629193534183728\n",
            "451th training loss is 0.6628561420374021\n",
            "452th training loss is 0.6627929414194418\n",
            "453th training loss is 0.6627297515620506\n",
            "454th training loss is 0.6626665724628008\n",
            "455th training loss is 0.6626034041192738\n",
            "456th training loss is 0.662540246529061\n",
            "457th training loss is 0.6624770996897654\n",
            "458th training loss is 0.6624139635989998\n",
            "459th training loss is 0.6623508382543861\n",
            "460th training loss is 0.6622877236535564\n",
            "461th training loss is 0.662224619794152\n",
            "462th training loss is 0.6621615266738242\n",
            "463th training loss is 0.6620984442902323\n",
            "464th training loss is 0.6620353726410461\n",
            "465th training loss is 0.6619723117239418\n",
            "466th training loss is 0.6619092615366063\n",
            "467th training loss is 0.6618462220767337\n",
            "468th training loss is 0.6617831933420274\n",
            "469th training loss is 0.6617201753301971\n",
            "470th training loss is 0.6616571680389625\n",
            "471th training loss is 0.6615941714660496\n",
            "472th training loss is 0.6615311856091923\n",
            "473th training loss is 0.6614682104661321\n",
            "474th training loss is 0.6614052460346179\n",
            "475th training loss is 0.661342292312405\n",
            "476th training loss is 0.6612793492972571\n",
            "477th training loss is 0.6612164169869431\n",
            "478th training loss is 0.6611534953792403\n",
            "479th training loss is 0.6610905844719307\n",
            "480th training loss is 0.6610276842628052\n",
            "481th training loss is 0.6609647947496582\n",
            "482th training loss is 0.6609019159302927\n",
            "483th training loss is 0.6608390478025161\n",
            "484th training loss is 0.660776190364143\n",
            "485th training loss is 0.6607133436129935\n",
            "486th training loss is 0.6606505075468926\n",
            "487th training loss is 0.6605876821636721\n",
            "488th training loss is 0.6605248674611682\n",
            "489th training loss is 0.6604620634372237\n",
            "490th training loss is 0.6603992700896852\n",
            "491th training loss is 0.6603364874164069\n",
            "492th training loss is 0.6602737154152449\n",
            "493th training loss is 0.6602109540840627\n",
            "494th training loss is 0.660148203420728\n",
            "495th training loss is 0.6600854634231127\n",
            "496th training loss is 0.6600227340890934\n",
            "497th training loss is 0.6599600154165536\n",
            "498th training loss is 0.6598973074033775\n",
            "499th training loss is 0.6598346100474571\n",
            "500th training loss is 0.6597719233466866\n",
            "501th training loss is 0.6597092472989654\n",
            "502th training loss is 0.6596465819021964\n",
            "503th training loss is 0.6595839271542879\n",
            "504th training loss is 0.6595212830531507\n",
            "505th training loss is 0.6594586495967011\n",
            "506th training loss is 0.6593960267828569\n",
            "507th training loss is 0.6593334146095423\n",
            "508th training loss is 0.6592708130746835\n",
            "509th training loss is 0.6592082221762109\n",
            "510th training loss is 0.6591456419120585\n",
            "511th training loss is 0.6590830722801637\n",
            "512th training loss is 0.6590205132784676\n",
            "513th training loss is 0.6589579649049133\n",
            "514th training loss is 0.65889542715745\n",
            "515th training loss is 0.6588329000340268\n",
            "516th training loss is 0.6587703835325985\n",
            "517th training loss is 0.6587078776511222\n",
            "518th training loss is 0.6586453823875573\n",
            "519th training loss is 0.6585828977398669\n",
            "520th training loss is 0.6585204237060176\n",
            "521th training loss is 0.6584579602839777\n",
            "522th training loss is 0.6583955074717189\n",
            "523th training loss is 0.6583330652672158\n",
            "524th training loss is 0.6582706336684453\n",
            "525th training loss is 0.6582082126733871\n",
            "526th training loss is 0.6581458022800242\n",
            "527th training loss is 0.6580834024863412\n",
            "528th training loss is 0.6580210132903255\n",
            "529th training loss is 0.657958634689967\n",
            "530th training loss is 0.6578962666832585\n",
            "531th training loss is 0.657833909268194\n",
            "532th training loss is 0.6577715624427717\n",
            "533th training loss is 0.6577092262049897\n",
            "534th training loss is 0.6576469005528504\n",
            "535th training loss is 0.6575845854843573\n",
            "536th training loss is 0.6575222809975166\n",
            "537th training loss is 0.6574599870903362\n",
            "538th training loss is 0.6573977037608257\n",
            "539th training loss is 0.6573354310069984\n",
            "540th training loss is 0.6572731688268678\n",
            "541th training loss is 0.6572109172184503\n",
            "542th training loss is 0.6571486761797637\n",
            "543th training loss is 0.6570864457088282\n",
            "544th training loss is 0.6570242258036662\n",
            "545th training loss is 0.6569620164622998\n",
            "546th training loss is 0.6568998176827562\n",
            "547th training loss is 0.6568376294630616\n",
            "548th training loss is 0.6567754518012453\n",
            "549th training loss is 0.6567132846953376\n",
            "550th training loss is 0.6566511281433715\n",
            "551th training loss is 0.6565889821433804\n",
            "552th training loss is 0.6565268466933991\n",
            "553th training loss is 0.6564647217914665\n",
            "554th training loss is 0.6564026074356201\n",
            "555th training loss is 0.6563405036238998\n",
            "556th training loss is 0.6562784103543484\n",
            "557th training loss is 0.6562163276250077\n",
            "558th training loss is 0.6561542554339234\n",
            "559th training loss is 0.6560921937791411\n",
            "560th training loss is 0.656030142658708\n",
            "561th training loss is 0.6559681020706727\n",
            "562th training loss is 0.6559060720130858\n",
            "563th training loss is 0.6558440524839987\n",
            "564th training loss is 0.6557820434814629\n",
            "565th training loss is 0.6557200450035339\n",
            "566th training loss is 0.6556580570482663\n",
            "567th training loss is 0.6555960796137166\n",
            "568th training loss is 0.6555341126979417\n",
            "569th training loss is 0.6554721562990018\n",
            "570th training loss is 0.655410210414956\n",
            "571th training loss is 0.6553482750438651\n",
            "572th training loss is 0.6552863501837922\n",
            "573th training loss is 0.6552244358328002\n",
            "574th training loss is 0.6551625319889536\n",
            "575th training loss is 0.6551006386503179\n",
            "576th training loss is 0.6550387558149598\n",
            "577th training loss is 0.6549768834809474\n",
            "578th training loss is 0.6549150216463479\n",
            "579th training loss is 0.654853170309232\n",
            "580th training loss is 0.6547913294676703\n",
            "581th training loss is 0.654729499119734\n",
            "582th training loss is 0.6546676792634959\n",
            "583th training loss is 0.6546058698970287\n",
            "584th training loss is 0.654544071018408\n",
            "585th training loss is 0.6544822826257082\n",
            "586th training loss is 0.6544205047170057\n",
            "587th training loss is 0.6543587372903776\n",
            "588th training loss is 0.6542969803439017\n",
            "589th training loss is 0.6542352338756569\n",
            "590th training loss is 0.6541734978837221\n",
            "591th training loss is 0.6541117723661787\n",
            "592th training loss is 0.6540500573211075\n",
            "593th training loss is 0.6539883527465905\n",
            "594th training loss is 0.6539266586407103\n",
            "595th training loss is 0.6538649750015506\n",
            "596th training loss is 0.6538033018271963\n",
            "597th training loss is 0.6537416391157317\n",
            "598th training loss is 0.6536799868652424\n",
            "599th training loss is 0.6536183450738154\n",
            "600th training loss is 0.6535567137395383\n",
            "601th training loss is 0.6534950928604988\n",
            "602th training loss is 0.653433482434785\n",
            "603th training loss is 0.653371882460487\n",
            "604th training loss is 0.653310292935694\n",
            "605th training loss is 0.6532487138584975\n",
            "606th training loss is 0.6531871452269883\n",
            "607th training loss is 0.6531255870392587\n",
            "608th training loss is 0.6530640392934006\n",
            "609th training loss is 0.6530025019875081\n",
            "610th training loss is 0.6529409751196749\n",
            "611th training loss is 0.6528794586879945\n",
            "612th training loss is 0.6528179526905631\n",
            "613th training loss is 0.6527564571254758\n",
            "614th training loss is 0.6526949719908286\n",
            "615th training loss is 0.6526334972847191\n",
            "616th training loss is 0.6525720330052436\n",
            "617th training loss is 0.6525105791505008\n",
            "618th training loss is 0.6524491357185894\n",
            "619th training loss is 0.6523877027076073\n",
            "620th training loss is 0.6523262801156554\n",
            "621th training loss is 0.6522648679408326\n",
            "622th training loss is 0.6522034661812403\n",
            "623th training loss is 0.6521420748349793\n",
            "624th training loss is 0.6520806939001517\n",
            "625th training loss is 0.652019323374859\n",
            "626th training loss is 0.6519579632572041\n",
            "627th training loss is 0.6518966135452902\n",
            "628th training loss is 0.6518352742372207\n",
            "629th training loss is 0.6517739453311002\n",
            "630th training loss is 0.6517126268250326\n",
            "631th training loss is 0.6516513187171233\n",
            "632th training loss is 0.651590021005478\n",
            "633th training loss is 0.6515287336882025\n",
            "634th training loss is 0.6514674567634035\n",
            "635th training loss is 0.6514061902291872\n",
            "636th training loss is 0.6513449340836611\n",
            "637th training loss is 0.6512836883249336\n",
            "638th training loss is 0.6512224529511116\n",
            "639th training loss is 0.6511612279603047\n",
            "640th training loss is 0.651100013350622\n",
            "641th training loss is 0.651038809120173\n",
            "642th training loss is 0.6509776152670663\n",
            "643th training loss is 0.6509164317894136\n",
            "644th training loss is 0.6508552586853245\n",
            "645th training loss is 0.6507940959529112\n",
            "646th training loss is 0.6507329435902838\n",
            "647th training loss is 0.6506718015955554\n",
            "648th training loss is 0.6506106699668374\n",
            "649th training loss is 0.6505495487022427\n",
            "650th training loss is 0.6504884377998846\n",
            "651th training loss is 0.6504273372578755\n",
            "652th training loss is 0.6503662470743304\n",
            "653th training loss is 0.6503051672473626\n",
            "654th training loss is 0.6502440977750867\n",
            "655th training loss is 0.650183038655618\n",
            "656th training loss is 0.6501219898870713\n",
            "657th training loss is 0.6500609514675619\n",
            "658th training loss is 0.6499999233952064\n",
            "659th training loss is 0.6499389056681202\n",
            "660th training loss is 0.6498778982844208\n",
            "661th training loss is 0.6498169012422245\n",
            "662th training loss is 0.6497559145396491\n",
            "663th training loss is 0.6496949381748112\n",
            "664th training loss is 0.6496339721458307\n",
            "665th training loss is 0.6495730164508235\n",
            "666th training loss is 0.6495120710879098\n",
            "667th training loss is 0.6494511360552085\n",
            "668th training loss is 0.6493902113508381\n",
            "669th training loss is 0.6493292969729186\n",
            "670th training loss is 0.64926839291957\n",
            "671th training loss is 0.6492074991889123\n",
            "672th training loss is 0.6491466157790663\n",
            "673th training loss is 0.6490857426881523\n",
            "674th training loss is 0.6490248799142921\n",
            "675th training loss is 0.648964027455607\n",
            "676th training loss is 0.6489031853102183\n",
            "677th training loss is 0.6488423534762486\n",
            "678th training loss is 0.64878153195182\n",
            "679th training loss is 0.6487207207350552\n",
            "680th training loss is 0.6486599198240774\n",
            "681th training loss is 0.6485991292170096\n",
            "682th training loss is 0.6485383489119748\n",
            "683th training loss is 0.6484775789070981\n",
            "684th training loss is 0.6484168192005024\n",
            "685th training loss is 0.6483560697903132\n",
            "686th training loss is 0.6482953306746548\n",
            "687th training loss is 0.6482346018516512\n",
            "688th training loss is 0.6481738833194292\n",
            "689th training loss is 0.6481131750761134\n",
            "690th training loss is 0.6480524771198295\n",
            "691th training loss is 0.6479917894487041\n",
            "692th training loss is 0.6479311120608641\n",
            "693th training loss is 0.6478704449544345\n",
            "694th training loss is 0.6478097881275436\n",
            "695th training loss is 0.6477491415783179\n",
            "696th training loss is 0.6476885053048853\n",
            "697th training loss is 0.6476278793053734\n",
            "698th training loss is 0.6475672635779098\n",
            "699th training loss is 0.6475066581206232\n",
            "700th training loss is 0.6474460629316418\n",
            "701th training loss is 0.6473854780090947\n",
            "702th training loss is 0.647324903351111\n",
            "703th training loss is 0.6472643389558194\n",
            "704th training loss is 0.6472037848213501\n",
            "705th training loss is 0.6471432409458326\n",
            "706th training loss is 0.6470827073273968\n",
            "707th training loss is 0.6470221839641738\n",
            "708th training loss is 0.6469616708542933\n",
            "709th training loss is 0.6469011679958866\n",
            "710th training loss is 0.6468406753870843\n",
            "711th training loss is 0.6467801930260185\n",
            "712th training loss is 0.6467197209108202\n",
            "713th training loss is 0.6466592590396215\n",
            "714th training loss is 0.6465988074105544\n",
            "715th training loss is 0.6465383660217514\n",
            "716th training loss is 0.6464779348713446\n",
            "717th training loss is 0.6464175139574674\n",
            "718th training loss is 0.6463571032782529\n",
            "719th training loss is 0.6462967028318336\n",
            "720th training loss is 0.6462363126163442\n",
            "721th training loss is 0.6461759326299178\n",
            "722th training loss is 0.6461155628706882\n",
            "723th training loss is 0.6460552033367899\n",
            "724th training loss is 0.6459948540263578\n",
            "725th training loss is 0.6459345149375263\n",
            "726th training loss is 0.645874186068431\n",
            "727th training loss is 0.6458138674172064\n",
            "728th training loss is 0.6457535589819885\n",
            "729th training loss is 0.6456932607609123\n",
            "730th training loss is 0.6456329727521145\n",
            "731th training loss is 0.645572694953731\n",
            "732th training loss is 0.6455124273638978\n",
            "733th training loss is 0.6454521699807523\n",
            "734th training loss is 0.6453919228024312\n",
            "735th training loss is 0.6453316858270713\n",
            "736th training loss is 0.64527145905281\n",
            "737th training loss is 0.6452112424777849\n",
            "738th training loss is 0.645151036100134\n",
            "739th training loss is 0.6450908399179953\n",
            "740th training loss is 0.645030653929507\n",
            "741th training loss is 0.6449704781328074\n",
            "742th training loss is 0.6449103125260359\n",
            "743th training loss is 0.644850157107331\n",
            "744th training loss is 0.644790011874832\n",
            "745th training loss is 0.6447298768266778\n",
            "746th training loss is 0.6446697519610088\n",
            "747th training loss is 0.6446096372759642\n",
            "748th training loss is 0.6445495327696849\n",
            "749th training loss is 0.64448943844031\n",
            "750th training loss is 0.6444293542859819\n",
            "751th training loss is 0.6443692803048392\n",
            "752th training loss is 0.6443092164950247\n",
            "753th training loss is 0.6442491628546784\n",
            "754th training loss is 0.6441891193819428\n",
            "755th training loss is 0.6441290860749587\n",
            "756th training loss is 0.6440690629318683\n",
            "757th training loss is 0.6440090499508137\n",
            "758th training loss is 0.6439490471299372\n",
            "759th training loss is 0.6438890544673818\n",
            "760th training loss is 0.6438290719612891\n",
            "761th training loss is 0.6437690996098037\n",
            "762th training loss is 0.643709137411068\n",
            "763th training loss is 0.6436491853632254\n",
            "764th training loss is 0.6435892434644198\n",
            "765th training loss is 0.6435293117127944\n",
            "766th training loss is 0.6434693901064943\n",
            "767th training loss is 0.6434094786436633\n",
            "768th training loss is 0.6433495773224461\n",
            "769th training loss is 0.6432896861409877\n",
            "770th training loss is 0.6432298050974324\n",
            "771th training loss is 0.6431699341899256\n",
            "772th training loss is 0.6431100734166134\n",
            "773th training loss is 0.6430502227756407\n",
            "774th training loss is 0.6429903822651541\n",
            "775th training loss is 0.6429305518832987\n",
            "776th training loss is 0.6428707316282216\n",
            "777th training loss is 0.642810921498069\n",
            "778th training loss is 0.6427511214909878\n",
            "779th training loss is 0.6426913316051244\n",
            "780th training loss is 0.6426315518386271\n",
            "781th training loss is 0.6425717821896424\n",
            "782th training loss is 0.6425120226563177\n",
            "783th training loss is 0.6424522732368015\n",
            "784th training loss is 0.6423925339292417\n",
            "785th training loss is 0.6423328047317859\n",
            "786th training loss is 0.6422730856425837\n",
            "787th training loss is 0.6422133766597831\n",
            "788th training loss is 0.6421536777815328\n",
            "789th training loss is 0.6420939890059824\n",
            "790th training loss is 0.6420343103312804\n",
            "791th training loss is 0.6419746417555777\n",
            "792th training loss is 0.6419149832770232\n",
            "793th training loss is 0.6418553348937671\n",
            "794th training loss is 0.6417956966039596\n",
            "795th training loss is 0.6417360684057506\n",
            "796th training loss is 0.6416764502972919\n",
            "797th training loss is 0.641616842276733\n",
            "798th training loss is 0.6415572443422257\n",
            "799th training loss is 0.6414976564919214\n",
            "800th training loss is 0.6414380787239714\n",
            "801th training loss is 0.6413785110365272\n",
            "802th training loss is 0.6413189534277405\n",
            "803th training loss is 0.641259405895764\n",
            "804th training loss is 0.6411998684387502\n",
            "805th training loss is 0.641140341054851\n",
            "806th training loss is 0.6410808237422193\n",
            "807th training loss is 0.6410213164990086\n",
            "808th training loss is 0.6409618193233719\n",
            "809th training loss is 0.6409023322134622\n",
            "810th training loss is 0.6408428551674333\n",
            "811th training loss is 0.6407833881834396\n",
            "812th training loss is 0.6407239312596342\n",
            "813th training loss is 0.6406644843941725\n",
            "814th training loss is 0.6406050475852082\n",
            "815th training loss is 0.6405456208308958\n",
            "816th training loss is 0.6404862041293914\n",
            "817th training loss is 0.6404267974788483\n",
            "818th training loss is 0.6403674008774239\n",
            "819th training loss is 0.640308014323272\n",
            "820th training loss is 0.6402486378145494\n",
            "821th training loss is 0.6401892713494116\n",
            "822th training loss is 0.6401299149260153\n",
            "823th training loss is 0.6400705685425165\n",
            "824th training loss is 0.6400112321970718\n",
            "825th training loss is 0.6399519058878387\n",
            "826th training loss is 0.6398925896129734\n",
            "827th training loss is 0.6398332833706332\n",
            "828th training loss is 0.639773987158976\n",
            "829th training loss is 0.6397147009761592\n",
            "830th training loss is 0.6396554248203415\n",
            "831th training loss is 0.6395961586896799\n",
            "832th training loss is 0.6395369025823333\n",
            "833th training loss is 0.6394776564964606\n",
            "834th training loss is 0.6394184204302195\n",
            "835th training loss is 0.6393591943817697\n",
            "836th training loss is 0.6392999783492704\n",
            "837th training loss is 0.6392407723308812\n",
            "838th training loss is 0.6391815763247609\n",
            "839th training loss is 0.6391223903290703\n",
            "840th training loss is 0.6390632143419688\n",
            "841th training loss is 0.6390040483616168\n",
            "842th training loss is 0.6389448923861746\n",
            "843th training loss is 0.6388857464138034\n",
            "844th training loss is 0.6388266104426636\n",
            "845th training loss is 0.6387674844709164\n",
            "846th training loss is 0.6387083684967236\n",
            "847th training loss is 0.6386492625182462\n",
            "848th training loss is 0.6385901665336461\n",
            "849th training loss is 0.6385310805410852\n",
            "850th training loss is 0.6384720045387259\n",
            "851th training loss is 0.6384129385247302\n",
            "852th training loss is 0.6383538824972611\n",
            "853th training loss is 0.6382948364544813\n",
            "854th training loss is 0.638235800394554\n",
            "855th training loss is 0.6381767743156415\n",
            "856th training loss is 0.6381177582159084\n",
            "857th training loss is 0.638058752093518\n",
            "858th training loss is 0.6379997559466343\n",
            "859th training loss is 0.6379407697734212\n",
            "860th training loss is 0.6378817935720431\n",
            "861th training loss is 0.6378228273406649\n",
            "862th training loss is 0.6377638710774501\n",
            "863th training loss is 0.637704924780565\n",
            "864th training loss is 0.637645988448174\n",
            "865th training loss is 0.6375870620784432\n",
            "866th training loss is 0.6375281456695375\n",
            "867th training loss is 0.6374692392196227\n",
            "868th training loss is 0.6374103427268658\n",
            "869th training loss is 0.637351456189432\n",
            "870th training loss is 0.6372925796054879\n",
            "871th training loss is 0.6372337129732006\n",
            "872th training loss is 0.637174856290737\n",
            "873th training loss is 0.637116009556264\n",
            "874th training loss is 0.6370571727679486\n",
            "875th training loss is 0.6369983459239587\n",
            "876th training loss is 0.6369395290224621\n",
            "877th training loss is 0.6368807220616263\n",
            "878th training loss is 0.6368219250396199\n",
            "879th training loss is 0.6367631379546113\n",
            "880th training loss is 0.6367043608047689\n",
            "881th training loss is 0.6366455935882617\n",
            "882th training loss is 0.6365868363032586\n",
            "883th training loss is 0.6365280889479289\n",
            "884th training loss is 0.6364693515204418\n",
            "885th training loss is 0.6364106240189674\n",
            "886th training loss is 0.6363519064416757\n",
            "887th training loss is 0.636293198786736\n",
            "888th training loss is 0.6362345010523195\n",
            "889th training loss is 0.6361758132365967\n",
            "890th training loss is 0.6361171353377373\n",
            "891th training loss is 0.6360584673539132\n",
            "892th training loss is 0.6359998092832954\n",
            "893th training loss is 0.6359411611240553\n",
            "894th training loss is 0.6358825228743652\n",
            "895th training loss is 0.6358238945323952\n",
            "896th training loss is 0.635765276096319\n",
            "897th training loss is 0.635706667564308\n",
            "898th training loss is 0.6356480689345348\n",
            "899th training loss is 0.6355894802051724\n",
            "900th training loss is 0.6355309013743934\n",
            "901th training loss is 0.6354723324403713\n",
            "902th training loss is 0.6354137734012787\n",
            "903th training loss is 0.6353552242552903\n",
            "904th training loss is 0.6352966850005788\n",
            "905th training loss is 0.6352381556353185\n",
            "906th training loss is 0.6351796361576837\n",
            "907th training loss is 0.6351211265658488\n",
            "908th training loss is 0.6350626268579889\n",
            "909th training loss is 0.6350041370322778\n",
            "910th training loss is 0.6349456570868915\n",
            "911th training loss is 0.6348871870200049\n",
            "912th training loss is 0.6348287268297939\n",
            "913th training loss is 0.6347702765144334\n",
            "914th training loss is 0.6347118360720999\n",
            "915th training loss is 0.6346534055009694\n",
            "916th training loss is 0.6345949847992188\n",
            "917th training loss is 0.6345365739650239\n",
            "918th training loss is 0.6344781729965618\n",
            "919th training loss is 0.6344197818920094\n",
            "920th training loss is 0.6343614006495445\n",
            "921th training loss is 0.6343030292673437\n",
            "922th training loss is 0.6342446677435855\n",
            "923th training loss is 0.6341863160764472\n",
            "924th training loss is 0.6341279742641072\n",
            "925th training loss is 0.6340696423047438\n",
            "926th training loss is 0.6340113201965352\n",
            "927th training loss is 0.6339530079376601\n",
            "928th training loss is 0.6338947055262987\n",
            "929th training loss is 0.6338364129606288\n",
            "930th training loss is 0.6337781302388302\n",
            "931th training loss is 0.6337198573590824\n",
            "932th training loss is 0.6336615943195658\n",
            "933th training loss is 0.6336033411184602\n",
            "934th training loss is 0.6335450977539455\n",
            "935th training loss is 0.633486864224203\n",
            "936th training loss is 0.6334286405274124\n",
            "937th training loss is 0.6333704266617558\n",
            "938th training loss is 0.633312222625413\n",
            "939th training loss is 0.6332540284165669\n",
            "940th training loss is 0.6331958440333973\n",
            "941th training loss is 0.6331376694740876\n",
            "942th training loss is 0.6330795047368191\n",
            "943th training loss is 0.6330213498197741\n",
            "944th training loss is 0.6329632047211349\n",
            "945th training loss is 0.6329050694390845\n",
            "946th training loss is 0.6328469439718057\n",
            "947th training loss is 0.632788828317482\n",
            "948th training loss is 0.6327307224742962\n",
            "949th training loss is 0.6326726264404319\n",
            "950th training loss is 0.6326145402140726\n",
            "951th training loss is 0.6325564637934032\n",
            "952th training loss is 0.632498397176607\n",
            "953th training loss is 0.6324403403618691\n",
            "954th training loss is 0.6323822933473736\n",
            "955th training loss is 0.6323242561313062\n",
            "956th training loss is 0.6322662287118508\n",
            "957th training loss is 0.6322082110871935\n",
            "958th training loss is 0.6321502032555201\n",
            "959th training loss is 0.6320922052150152\n",
            "960th training loss is 0.6320342169638661\n",
            "961th training loss is 0.6319762385002582\n",
            "962th training loss is 0.6319182698223778\n",
            "963th training loss is 0.6318603109284123\n",
            "964th training loss is 0.631802361816548\n",
            "965th training loss is 0.6317444224849718\n",
            "966th training loss is 0.6316864929318713\n",
            "967th training loss is 0.6316285731554335\n",
            "968th training loss is 0.6315706631538472\n",
            "969th training loss is 0.6315127629252992\n",
            "970th training loss is 0.6314548724679783\n",
            "971th training loss is 0.6313969917800731\n",
            "972th training loss is 0.6313391208597715\n",
            "973th training loss is 0.6312812597052628\n",
            "974th training loss is 0.6312234083147354\n",
            "975th training loss is 0.6311655666863802\n",
            "976th training loss is 0.6311077348183846\n",
            "977th training loss is 0.6310499127089397\n",
            "978th training loss is 0.6309921003562347\n",
            "979th training loss is 0.6309342977584608\n",
            "980th training loss is 0.6308765049138072\n",
            "981th training loss is 0.630818721820465\n",
            "982th training loss is 0.6307609484766248\n",
            "983th training loss is 0.630703184880478\n",
            "984th training loss is 0.6306454310302158\n",
            "985th training loss is 0.6305876869240293\n",
            "986th training loss is 0.6305299525601105\n",
            "987th training loss is 0.6304722279366513\n",
            "988th training loss is 0.630414513051844\n",
            "989th training loss is 0.6303568079038805\n",
            "990th training loss is 0.6302991124909538\n",
            "991th training loss is 0.6302414268112564\n",
            "992th training loss is 0.6301837508629817\n",
            "993th training loss is 0.6301260846443226\n",
            "994th training loss is 0.6300684281534726\n",
            "995th training loss is 0.6300107813886261\n",
            "996th training loss is 0.6299531443479762\n",
            "997th training loss is 0.6298955170297166\n",
            "998th training loss is 0.6298378994320427\n",
            "999th training loss is 0.6297802915531489\n",
            "1000th training loss is 0.6297226933912299\n",
            "1001th training loss is 0.6296651049444806\n",
            "1002th training loss is 0.6296075262110961\n",
            "1003th training loss is 0.6295499571892726\n",
            "1004th training loss is 0.6294923978772047\n",
            "1005th training loss is 0.6294348482730889\n",
            "1006th training loss is 0.6293773083751213\n",
            "1007th training loss is 0.6293197781814989\n",
            "1008th training loss is 0.6292622576904172\n",
            "1009th training loss is 0.6292047469000734\n",
            "1010th training loss is 0.6291472458086648\n",
            "1011th training loss is 0.6290897544143886\n",
            "1012th training loss is 0.6290322727154419\n",
            "1013th training loss is 0.6289748007100227\n",
            "1014th training loss is 0.6289173383963289\n",
            "1015th training loss is 0.628859885772559\n",
            "1016th training loss is 0.6288024428369104\n",
            "1017th training loss is 0.6287450095875824\n",
            "1018th training loss is 0.628687586022774\n",
            "1019th training loss is 0.6286301721406837\n",
            "1020th training loss is 0.6285727679395112\n",
            "1021th training loss is 0.6285153734174557\n",
            "1022th training loss is 0.6284579885727171\n",
            "1023th training loss is 0.6284006134034954\n",
            "1024th training loss is 0.6283432479079903\n",
            "1025th training loss is 0.6282858920844026\n",
            "1026th training loss is 0.6282285459309327\n",
            "1027th training loss is 0.6281712094457818\n",
            "1028th training loss is 0.6281138826271504\n",
            "1029th training loss is 0.6280565654732401\n",
            "1030th training loss is 0.6279992579822525\n",
            "1031th training loss is 0.6279419601523895\n",
            "1032th training loss is 0.6278846719818522\n",
            "1033th training loss is 0.6278273934688438\n",
            "1034th training loss is 0.6277701246115654\n",
            "1035th training loss is 0.6277128654082214\n",
            "1036th training loss is 0.6276556158570135\n",
            "1037th training loss is 0.6275983759561445\n",
            "1038th training loss is 0.6275411457038188\n",
            "1039th training loss is 0.6274839250982389\n",
            "1040th training loss is 0.6274267141376093\n",
            "1041th training loss is 0.6273695128201335\n",
            "1042th training loss is 0.6273123211440158\n",
            "1043th training loss is 0.6272551391074609\n",
            "1044th training loss is 0.6271979667086733\n",
            "1045th training loss is 0.6271408039458569\n",
            "1046th training loss is 0.6270836508172184\n",
            "1047th training loss is 0.6270265073209625\n",
            "1048th training loss is 0.6269693734552941\n",
            "1049th training loss is 0.62691224921842\n",
            "1050th training loss is 0.6268551346085458\n",
            "1051th training loss is 0.6267980296238777\n",
            "1052th training loss is 0.6267409342626218\n",
            "1053th training loss is 0.6266838485229852\n",
            "1054th training loss is 0.6266267724031748\n",
            "1055th training loss is 0.6265697059013979\n",
            "1056th training loss is 0.6265126490158607\n",
            "1057th training loss is 0.6264556017447728\n",
            "1058th training loss is 0.6263985640863406\n",
            "1059th training loss is 0.6263415360387719\n",
            "1060th training loss is 0.6262845176002761\n",
            "1061th training loss is 0.6262275087690607\n",
            "1062th training loss is 0.6261705095433348\n",
            "1063th training loss is 0.6261135199213079\n",
            "1064th training loss is 0.6260565399011879\n",
            "1065th training loss is 0.6259995694811854\n",
            "1066th training loss is 0.6259426086595096\n",
            "1067th training loss is 0.6258856574343697\n",
            "1068th training loss is 0.6258287158039769\n",
            "1069th training loss is 0.6257717837665406\n",
            "1070th training loss is 0.6257148613202719\n",
            "1071th training loss is 0.6256579484633809\n",
            "1072th training loss is 0.6256010451940794\n",
            "1073th training loss is 0.625544151510578\n",
            "1074th training loss is 0.6254872674110885\n",
            "1075th training loss is 0.6254303928938225\n",
            "1076th training loss is 0.6253735279569916\n",
            "1077th training loss is 0.625316672598808\n",
            "1078th training loss is 0.6252598268174846\n",
            "1079th training loss is 0.6252029906112333\n",
            "1080th training loss is 0.6251461639782668\n",
            "1081th training loss is 0.6250893469167988\n",
            "1082th training loss is 0.625032539425042\n",
            "1083th training loss is 0.6249757415012106\n",
            "1084th training loss is 0.6249189531435173\n",
            "1085th training loss is 0.6248621743501768\n",
            "1086th training loss is 0.6248054051194029\n",
            "1087th training loss is 0.6247486454494102\n",
            "1088th training loss is 0.624691895338413\n",
            "1089th training loss is 0.6246351547846265\n",
            "1090th training loss is 0.6245784237862655\n",
            "1091th training loss is 0.6245217023415457\n",
            "1092th training loss is 0.6244649904486821\n",
            "1093th training loss is 0.6244082881058911\n",
            "1094th training loss is 0.624351595311388\n",
            "1095th training loss is 0.6242949120633897\n",
            "1096th training loss is 0.6242382383601118\n",
            "1097th training loss is 0.624181574199772\n",
            "1098th training loss is 0.6241249195805861\n",
            "1099th training loss is 0.6240682745007724\n",
            "1100th training loss is 0.6240116389585473\n",
            "1101th training loss is 0.6239550129521287\n",
            "1102th training loss is 0.6238983964797344\n",
            "1103th training loss is 0.6238417895395831\n",
            "1104th training loss is 0.6237851921298924\n",
            "1105th training loss is 0.6237286042488808\n",
            "1106th training loss is 0.6236720258947671\n",
            "1107th training loss is 0.6236154570657697\n",
            "1108th training loss is 0.6235588977601093\n",
            "1109th training loss is 0.6235023479760037\n",
            "1110th training loss is 0.6234458077116737\n",
            "1111th training loss is 0.6233892769653386\n",
            "1112th training loss is 0.6233327557352187\n",
            "1113th training loss is 0.6232762440195346\n",
            "1114th training loss is 0.623219741816506\n",
            "1115th training loss is 0.6231632491243548\n",
            "1116th training loss is 0.623106765941301\n",
            "1117th training loss is 0.6230502922655671\n",
            "1118th training loss is 0.622993828095373\n",
            "1119th training loss is 0.622937373428942\n",
            "1120th training loss is 0.622880928264495\n",
            "1121th training loss is 0.6228244926002543\n",
            "1122th training loss is 0.6227680664344428\n",
            "1123th training loss is 0.6227116497652826\n",
            "1124th training loss is 0.6226552425909969\n",
            "1125th training loss is 0.6225988449098085\n",
            "1126th training loss is 0.6225424567199415\n",
            "1127th training loss is 0.6224860780196183\n",
            "1128th training loss is 0.6224297088070634\n",
            "1129th training loss is 0.6223733490805009\n",
            "1130th training loss is 0.6223169988381549\n",
            "1131th training loss is 0.6222606580782495\n",
            "1132th training loss is 0.6222043267990102\n",
            "1133th training loss is 0.622148004998661\n",
            "1134th training loss is 0.6220916926754279\n",
            "1135th training loss is 0.6220353898275357\n",
            "1136th training loss is 0.6219790964532106\n",
            "1137th training loss is 0.6219228125506779\n",
            "1138th training loss is 0.6218665381181641\n",
            "1139th training loss is 0.6218102731538954\n",
            "1140th training loss is 0.6217540176560982\n",
            "1141th training loss is 0.6216977716229999\n",
            "1142th training loss is 0.6216415350528266\n",
            "1143th training loss is 0.6215853079438058\n",
            "1144th training loss is 0.6215290902941655\n",
            "1145th training loss is 0.6214728821021333\n",
            "1146th training loss is 0.6214166833659367\n",
            "1147th training loss is 0.6213604940838039\n",
            "1148th training loss is 0.6213043142539639\n",
            "1149th training loss is 0.621248143874645\n",
            "1150th training loss is 0.6211919829440761\n",
            "1151th training loss is 0.6211358314604858\n",
            "1152th training loss is 0.6210796894221046\n",
            "1153th training loss is 0.6210235568271607\n",
            "1154th training loss is 0.6209674336738847\n",
            "1155th training loss is 0.6209113199605071\n",
            "1156th training loss is 0.6208552156852571\n",
            "1157th training loss is 0.6207991208463653\n",
            "1158th training loss is 0.6207430354420636\n",
            "1159th training loss is 0.6206869594705816\n",
            "1160th training loss is 0.6206308929301513\n",
            "1161th training loss is 0.6205748358190035\n",
            "1162th training loss is 0.6205187881353704\n",
            "1163th training loss is 0.6204627498774836\n",
            "1164th training loss is 0.6204067210435757\n",
            "1165th training loss is 0.6203507016318784\n",
            "1166th training loss is 0.6202946916406248\n",
            "1167th training loss is 0.6202386910680472\n",
            "1168th training loss is 0.6201826999123793\n",
            "1169th training loss is 0.6201267181718543\n",
            "1170th training loss is 0.6200707458447048\n",
            "1171th training loss is 0.6200147829291652\n",
            "1172th training loss is 0.6199588294234698\n",
            "1173th training loss is 0.6199028853258522\n",
            "1174th training loss is 0.6198469506345478\n",
            "1175th training loss is 0.6197910253477901\n",
            "1176th training loss is 0.619735109463815\n",
            "1177th training loss is 0.6196792029808571\n",
            "1178th training loss is 0.6196233058971514\n",
            "1179th training loss is 0.6195674182109342\n",
            "1180th training loss is 0.6195115399204414\n",
            "1181th training loss is 0.6194556710239086\n",
            "1182th training loss is 0.6193998115195729\n",
            "1183th training loss is 0.6193439614056699\n",
            "1184th training loss is 0.619288120680437\n",
            "1185th training loss is 0.6192322893421107\n",
            "1186th training loss is 0.619176467388929\n",
            "1187th training loss is 0.6191206548191287\n",
            "1188th training loss is 0.6190648516309478\n",
            "1189th training loss is 0.6190090578226243\n",
            "1190th training loss is 0.6189532733923961\n",
            "1191th training loss is 0.618897498338502\n",
            "1192th training loss is 0.6188417326591803\n",
            "1193th training loss is 0.6187859763526703\n",
            "1194th training loss is 0.618730229417211\n",
            "1195th training loss is 0.6186744918510416\n",
            "1196th training loss is 0.6186187636524018\n",
            "1197th training loss is 0.6185630448195313\n",
            "1198th training loss is 0.61850733535067\n",
            "1199th training loss is 0.6184516352440587\n",
            "1200th training loss is 0.6183959444979377\n",
            "1201th training loss is 0.6183402631105476\n",
            "1202th training loss is 0.6182845910801298\n",
            "1203th training loss is 0.618228928404925\n",
            "1204th training loss is 0.6181732750831751\n",
            "1205th training loss is 0.6181176311131217\n",
            "1206th training loss is 0.6180619964930069\n",
            "1207th training loss is 0.6180063712210716\n",
            "1208th training loss is 0.6179507552955604\n",
            "1209th training loss is 0.6178951487147143\n",
            "1210th training loss is 0.6178395514767769\n",
            "1211th training loss is 0.617783963579991\n",
            "1212th training loss is 0.6177283850225997\n",
            "1213th training loss is 0.6176728158028477\n",
            "1214th training loss is 0.6176172559189773\n",
            "1215th training loss is 0.6175617053692332\n",
            "1216th training loss is 0.6175061641518602\n",
            "1217th training loss is 0.6174506322651026\n",
            "1218th training loss is 0.6173951097072047\n",
            "1219th training loss is 0.6173395964764119\n",
            "1220th training loss is 0.6172840925709692\n",
            "1221th training loss is 0.6172285979891223\n",
            "1222th training loss is 0.6171731127291166\n",
            "1223th training loss is 0.6171176367891982\n",
            "1224th training loss is 0.6170621701676136\n",
            "1225th training loss is 0.6170067128626084\n",
            "1226th training loss is 0.6169512648724299\n",
            "1227th training loss is 0.6168958261953252\n",
            "1228th training loss is 0.6168403968295405\n",
            "1229th training loss is 0.6167849767733242\n",
            "1230th training loss is 0.616729566024923\n",
            "1231th training loss is 0.6166741645825855\n",
            "1232th training loss is 0.616618772444559\n",
            "1233th training loss is 0.6165633896090921\n",
            "1234th training loss is 0.6165080160744335\n",
            "1235th training loss is 0.6164526518388322\n",
            "1236th training loss is 0.616397296900537\n",
            "1237th training loss is 0.6163419512577963\n",
            "1238th training loss is 0.6162866149088609\n",
            "1239th training loss is 0.6162312878519794\n",
            "1240th training loss is 0.616175970085403\n",
            "1241th training loss is 0.6161206616073808\n",
            "1242th training loss is 0.6160653624161635\n",
            "1243th training loss is 0.6160100725100022\n",
            "1244th training loss is 0.6159547918871472\n",
            "1245th training loss is 0.6158995205458496\n",
            "1246th training loss is 0.6158442584843618\n",
            "1247th training loss is 0.6157890057009341\n",
            "1248th training loss is 0.6157337621938195\n",
            "1249th training loss is 0.6156785279612691\n",
            "1250th training loss is 0.6156233030015354\n",
            "1251th training loss is 0.6155680873128718\n",
            "1252th training loss is 0.6155128808935304\n",
            "1253th training loss is 0.615457683741764\n",
            "1254th training loss is 0.6154024958558266\n",
            "1255th training loss is 0.615347317233971\n",
            "1256th training loss is 0.6152921478744515\n",
            "1257th training loss is 0.6152369877755224\n",
            "1258th training loss is 0.6151818369354362\n",
            "1259th training loss is 0.6151266953524491\n",
            "1260th training loss is 0.6150715630248159\n",
            "1261th training loss is 0.6150164399507898\n",
            "1262th training loss is 0.6149613261286275\n",
            "1263th training loss is 0.6149062215565841\n",
            "1264th training loss is 0.6148511262329152\n",
            "1265th training loss is 0.6147960401558763\n",
            "1266th training loss is 0.6147409633237237\n",
            "1267th training loss is 0.6146858957347141\n",
            "1268th training loss is 0.6146308373871041\n",
            "1269th training loss is 0.6145757882791498\n",
            "1270th training loss is 0.6145207484091093\n",
            "1271th training loss is 0.6144657177752392\n",
            "1272th training loss is 0.6144106963757975\n",
            "1273th training loss is 0.6143556842090417\n",
            "1274th training loss is 0.6143006812732293\n",
            "1275th training loss is 0.6142456875666199\n",
            "1276th training loss is 0.6141907030874711\n",
            "1277th training loss is 0.6141357278340414\n",
            "1278th training loss is 0.6140807618045907\n",
            "1279th training loss is 0.6140258049973775\n",
            "1280th training loss is 0.6139708574106615\n",
            "1281th training loss is 0.6139159190427027\n",
            "1282th training loss is 0.6138609898917603\n",
            "1283th training loss is 0.613806069956095\n",
            "1284th training loss is 0.6137511592339667\n",
            "1285th training loss is 0.6136962577236376\n",
            "1286th training loss is 0.6136413654233664\n",
            "1287th training loss is 0.6135864823314159\n",
            "1288th training loss is 0.6135316084460468\n",
            "1289th training loss is 0.6134767437655204\n",
            "1290th training loss is 0.6134218882880993\n",
            "1291th training loss is 0.6133670420120451\n",
            "1292th training loss is 0.6133122049356203\n",
            "1293th training loss is 0.6132573770570875\n",
            "1294th training loss is 0.6132025583747094\n",
            "1295th training loss is 0.6131477488867492\n",
            "1296th training loss is 0.6130929485914698\n",
            "1297th training loss is 0.6130381574871347\n",
            "1298th training loss is 0.6129833755720085\n",
            "1299th training loss is 0.6129286028443547\n",
            "1300th training loss is 0.6128738393024371\n",
            "1301th training loss is 0.6128190849445209\n",
            "1302th training loss is 0.6127643397688706\n",
            "1303th training loss is 0.6127096037737509\n",
            "1304th training loss is 0.6126548769574272\n",
            "1305th training loss is 0.612600159318165\n",
            "1306th training loss is 0.6125454508542297\n",
            "1307th training loss is 0.6124907515638879\n",
            "1308th training loss is 0.6124360614454046\n",
            "1309th training loss is 0.6123813804970476\n",
            "1310th training loss is 0.6123267087170825\n",
            "1311th training loss is 0.612272046103777\n",
            "1312th training loss is 0.6122173926553969\n",
            "1313th training loss is 0.612162748370211\n",
            "1314th training loss is 0.6121081132464863\n",
            "1315th training loss is 0.6120534872824905\n",
            "1316th training loss is 0.6119988704764922\n",
            "1317th training loss is 0.6119442628267588\n",
            "1318th training loss is 0.6118896643315599\n",
            "1319th training loss is 0.6118350749891637\n",
            "1320th training loss is 0.6117804947978395\n",
            "1321th training loss is 0.6117259237558562\n",
            "1322th training loss is 0.6116713618614843\n",
            "1323th training loss is 0.611616809112992\n",
            "1324th training loss is 0.6115622655086507\n",
            "1325th training loss is 0.6115077310467305\n",
            "1326th training loss is 0.6114532057255013\n",
            "1327th training loss is 0.6113986895432338\n",
            "1328th training loss is 0.6113441824981996\n",
            "1329th training loss is 0.6112896845886691\n",
            "1330th training loss is 0.6112351958129146\n",
            "1331th training loss is 0.6111807161692072\n",
            "1332th training loss is 0.6111262456558192\n",
            "1333th training loss is 0.6110717842710228\n",
            "1334th training loss is 0.6110173320130902\n",
            "1335th training loss is 0.6109628888802939\n",
            "1336th training loss is 0.6109084548709075\n",
            "1337th training loss is 0.610854029983203\n",
            "1338th training loss is 0.6107996142154547\n",
            "1339th training loss is 0.6107452075659364\n",
            "1340th training loss is 0.6106908100329211\n",
            "1341th training loss is 0.6106364216146833\n",
            "1342th training loss is 0.6105820423094981\n",
            "1343th training loss is 0.6105276721156387\n",
            "1344th training loss is 0.6104733110313808\n",
            "1345th training loss is 0.6104189590549994\n",
            "1346th training loss is 0.6103646161847699\n",
            "1347th training loss is 0.6103102824189677\n",
            "1348th training loss is 0.6102559577558686\n",
            "1349th training loss is 0.610201642193748\n",
            "1350th training loss is 0.6101473357308835\n",
            "1351th training loss is 0.6100930383655511\n",
            "1352th training loss is 0.6100387500960271\n",
            "1353th training loss is 0.609984470920589\n",
            "1354th training loss is 0.6099302008375137\n",
            "1355th training loss is 0.6098759398450788\n",
            "1356th training loss is 0.6098216879415627\n",
            "1357th training loss is 0.6097674451252423\n",
            "1358th training loss is 0.6097132113943965\n",
            "1359th training loss is 0.6096589867473039\n",
            "1360th training loss is 0.6096047711822427\n",
            "1361th training loss is 0.6095505646974916\n",
            "1362th training loss is 0.6094963672913308\n",
            "1363th training loss is 0.6094421789620392\n",
            "1364th training loss is 0.6093879997078965\n",
            "1365th training loss is 0.6093338295271825\n",
            "1366th training loss is 0.6092796684181774\n",
            "1367th training loss is 0.6092255163791617\n",
            "1368th training loss is 0.6091713734084157\n",
            "1369th training loss is 0.6091172395042211\n",
            "1370th training loss is 0.6090631146648586\n",
            "1371th training loss is 0.609008998888609\n",
            "1372th training loss is 0.608954892173755\n",
            "1373th training loss is 0.6089007945185778\n",
            "1374th training loss is 0.6088467059213594\n",
            "1375th training loss is 0.6087926263803826\n",
            "1376th training loss is 0.6087385558939293\n",
            "1377th training loss is 0.6086844944602834\n",
            "1378th training loss is 0.608630442077727\n",
            "1379th training loss is 0.608576398744544\n",
            "1380th training loss is 0.6085223644590174\n",
            "1381th training loss is 0.6084683392194317\n",
            "1382th training loss is 0.6084143230240705\n",
            "1383th training loss is 0.6083603158712186\n",
            "1384th training loss is 0.6083063177591594\n",
            "1385th training loss is 0.6082523286861792\n",
            "1386th training loss is 0.6081983486505618\n",
            "1387th training loss is 0.608144377650593\n",
            "1388th training loss is 0.6080904156845581\n",
            "1389th training loss is 0.6080364627507437\n",
            "1390th training loss is 0.6079825188474343\n",
            "1391th training loss is 0.6079285839729173\n",
            "1392th training loss is 0.6078746581254785\n",
            "1393th training loss is 0.6078207413034052\n",
            "1394th training loss is 0.6077668335049847\n",
            "1395th training loss is 0.607712934728503\n",
            "1396th training loss is 0.6076590449722481\n",
            "1397th training loss is 0.6076051642345086\n",
            "1398th training loss is 0.6075512925135712\n",
            "1399th training loss is 0.6074974298077247\n",
            "1400th training loss is 0.6074435761152572\n",
            "1401th training loss is 0.6073897314344577\n",
            "1402th training loss is 0.6073358957636154\n",
            "1403th training loss is 0.6072820691010187\n",
            "1404th training loss is 0.6072282514449575\n",
            "1405th training loss is 0.6071744427937212\n",
            "1406th training loss is 0.6071206431456004\n",
            "1407th training loss is 0.6070668524988847\n",
            "1408th training loss is 0.6070130708518641\n",
            "1409th training loss is 0.6069592982028295\n",
            "1410th training loss is 0.6069055345500727\n",
            "1411th training loss is 0.6068517798918835\n",
            "1412th training loss is 0.6067980342265541\n",
            "1413th training loss is 0.6067442975523758\n",
            "1414th training loss is 0.6066905698676409\n",
            "1415th training loss is 0.6066368511706407\n",
            "1416th training loss is 0.6065831414596679\n",
            "1417th training loss is 0.606529440733016\n",
            "1418th training loss is 0.6064757489889764\n",
            "1419th training loss is 0.6064220662258427\n",
            "1420th training loss is 0.6063683924419082\n",
            "1421th training loss is 0.6063147276354675\n",
            "1422th training loss is 0.6062610718048127\n",
            "1423th training loss is 0.6062074249482393\n",
            "1424th training loss is 0.6061537870640404\n",
            "1425th training loss is 0.6061001581505118\n",
            "1426th training loss is 0.6060465382059474\n",
            "1427th training loss is 0.6059929272286423\n",
            "1428th training loss is 0.6059393252168923\n",
            "1429th training loss is 0.6058857321689927\n",
            "1430th training loss is 0.6058321480832389\n",
            "1431th training loss is 0.6057785729579276\n",
            "1432th training loss is 0.6057250067913544\n",
            "1433th training loss is 0.6056714495818167\n",
            "1434th training loss is 0.6056179013276105\n",
            "1435th training loss is 0.6055643620270331\n",
            "1436th training loss is 0.6055108316783817\n",
            "1437th training loss is 0.6054573102799533\n",
            "1438th training loss is 0.6054037978300463\n",
            "1439th training loss is 0.6053502943269589\n",
            "1440th training loss is 0.6052967997689889\n",
            "1441th training loss is 0.6052433141544343\n",
            "1442th training loss is 0.605189837481595\n",
            "1443th training loss is 0.6051363697487692\n",
            "1444th training loss is 0.6050829109542559\n",
            "1445th training loss is 0.6050294610963554\n",
            "1446th training loss is 0.6049760201733664\n",
            "1447th training loss is 0.6049225881835901\n",
            "1448th training loss is 0.6048691651253254\n",
            "1449th training loss is 0.6048157509968742\n",
            "1450th training loss is 0.6047623457965358\n",
            "1451th training loss is 0.6047089495226112\n",
            "1452th training loss is 0.6046555621734028\n",
            "1453th training loss is 0.6046021837472112\n",
            "1454th training loss is 0.6045488142423379\n",
            "1455th training loss is 0.6044954536570856\n",
            "1456th training loss is 0.604442101989756\n",
            "1457th training loss is 0.6043887592386512\n",
            "1458th training loss is 0.6043354254020744\n",
            "1459th training loss is 0.6042821004783284\n",
            "1460th training loss is 0.604228784465716\n",
            "1461th training loss is 0.6041754773625413\n",
            "1462th training loss is 0.604122179167107\n",
            "1463th training loss is 0.6040688898777182\n",
            "1464th training loss is 0.604015609492678\n",
            "1465th training loss is 0.6039623380102911\n",
            "1466th training loss is 0.6039090754288624\n",
            "1467th training loss is 0.6038558217466964\n",
            "1468th training loss is 0.6038025769620985\n",
            "1469th training loss is 0.6037493410733742\n",
            "1470th training loss is 0.6036961140788291\n",
            "1471th training loss is 0.6036428959767687\n",
            "1472th training loss is 0.6035896867654993\n",
            "1473th training loss is 0.6035364864433274\n",
            "1474th training loss is 0.6034832950085596\n",
            "1475th training loss is 0.6034301124595023\n",
            "1476th training loss is 0.6033769387944632\n",
            "1477th training loss is 0.6033237740117497\n",
            "1478th training loss is 0.6032706181096691\n",
            "1479th training loss is 0.6032174710865292\n",
            "1480th training loss is 0.6031643329406381\n",
            "1481th training loss is 0.6031112036703046\n",
            "1482th training loss is 0.6030580832738366\n",
            "1483th training loss is 0.6030049717495436\n",
            "1484th training loss is 0.6029518690957342\n",
            "1485th training loss is 0.6028987753107181\n",
            "1486th training loss is 0.6028456903928044\n",
            "1487th training loss is 0.6027926143403035\n",
            "1488th training loss is 0.602739547151525\n",
            "1489th training loss is 0.6026864888247793\n",
            "1490th training loss is 0.6026334393583772\n",
            "1491th training loss is 0.6025803987506299\n",
            "1492th training loss is 0.6025273669998474\n",
            "1493th training loss is 0.6024743441043422\n",
            "1494th training loss is 0.6024213300624248\n",
            "1495th training loss is 0.6023683248724074\n",
            "1496th training loss is 0.6023153285326023\n",
            "1497th training loss is 0.6022623410413215\n",
            "1498th training loss is 0.6022093623968781\n",
            "1499th training loss is 0.602156392597584\n",
            "1500th training loss is 0.6021034316417532\n",
            "1501th training loss is 0.6020504795276985\n",
            "1502th training loss is 0.6019975362537336\n",
            "1503th training loss is 0.6019446018181718\n",
            "1504th training loss is 0.6018916762193279\n",
            "1505th training loss is 0.6018387594555153\n",
            "1506th training loss is 0.6017858515250499\n",
            "1507th training loss is 0.601732952426245\n",
            "1508th training loss is 0.6016800621574168\n",
            "1509th training loss is 0.6016271807168798\n",
            "1510th training loss is 0.6015743081029497\n",
            "1511th training loss is 0.6015214443139426\n",
            "1512th training loss is 0.601468589348174\n",
            "1513th training loss is 0.6014157432039607\n",
            "1514th training loss is 0.6013629058796192\n",
            "1515th training loss is 0.601310077373466\n",
            "1516th training loss is 0.6012572576838182\n",
            "1517th training loss is 0.6012044468089932\n",
            "1518th training loss is 0.6011516447473081\n",
            "1519th training loss is 0.6010988514970813\n",
            "1520th training loss is 0.6010460670566303\n",
            "1521th training loss is 0.6009932914242737\n",
            "1522th training loss is 0.6009405245983298\n",
            "1523th training loss is 0.6008877665771177\n",
            "1524th training loss is 0.6008350173589562\n",
            "1525th training loss is 0.6007822769421644\n",
            "1526th training loss is 0.600729545325062\n",
            "1527th training loss is 0.6006768225059681\n",
            "1528th training loss is 0.6006241084832038\n",
            "1529th training loss is 0.6005714032550888\n",
            "1530th training loss is 0.6005187068199437\n",
            "1531th training loss is 0.600466019176089\n",
            "1532th training loss is 0.6004133403218467\n",
            "1533th training loss is 0.6003606702555367\n",
            "1534th training loss is 0.600308008975481\n",
            "1535th training loss is 0.6002553564800013\n",
            "1536th training loss is 0.6002027127674201\n",
            "1537th training loss is 0.6001500778360593\n",
            "1538th training loss is 0.6000974516842412\n",
            "1539th training loss is 0.6000448343102889\n",
            "1540th training loss is 0.5999922257125251\n",
            "1541th training loss is 0.5999396258892734\n",
            "1542th training loss is 0.5998870348388565\n",
            "1543th training loss is 0.599834452559599\n",
            "1544th training loss is 0.5997818790498248\n",
            "1545th training loss is 0.5997293143078583\n",
            "1546th training loss is 0.5996767583320229\n",
            "1547th training loss is 0.5996242111206446\n",
            "1548th training loss is 0.599571672672048\n",
            "1549th training loss is 0.5995191429845578\n",
            "1550th training loss is 0.5994666220565005\n",
            "1551th training loss is 0.5994141098862009\n",
            "1552th training loss is 0.5993616064719852\n",
            "1553th training loss is 0.5993091118121799\n",
            "1554th training loss is 0.5992566259051121\n",
            "1555th training loss is 0.5992041487491071\n",
            "1556th training loss is 0.5991516803424927\n",
            "1557th training loss is 0.5990992206835963\n",
            "1558th training loss is 0.5990467697707453\n",
            "1559th training loss is 0.5989943276022671\n",
            "1560th training loss is 0.5989418941764898\n",
            "1561th training loss is 0.5988894694917419\n",
            "1562th training loss is 0.5988370535463521\n",
            "1563th training loss is 0.5987846463386484\n",
            "1564th training loss is 0.5987322478669603\n",
            "1565th training loss is 0.5986798581296167\n",
            "1566th training loss is 0.5986274771249477\n",
            "1567th training loss is 0.5985751048512821\n",
            "1568th training loss is 0.5985227413069506\n",
            "1569th training loss is 0.598470386490284\n",
            "1570th training loss is 0.5984180403996114\n",
            "1571th training loss is 0.5983657030332644\n",
            "1572th training loss is 0.598313374389574\n",
            "1573th training loss is 0.5982610544668714\n",
            "1574th training loss is 0.5982087432634878\n",
            "1575th training loss is 0.5981564407777549\n",
            "1576th training loss is 0.5981041470080052\n",
            "1577th training loss is 0.5980518619525707\n",
            "1578th training loss is 0.5979995856097835\n",
            "1579th training loss is 0.5979473179779768\n",
            "1580th training loss is 0.5978950590554839\n",
            "1581th training loss is 0.5978428088406375\n",
            "1582th training loss is 0.5977905673317709\n",
            "1583th training loss is 0.5977383345272186\n",
            "1584th training loss is 0.597686110425314\n",
            "1585th training loss is 0.5976338950243916\n",
            "1586th training loss is 0.5975816883227861\n",
            "1587th training loss is 0.5975294903188317\n",
            "1588th training loss is 0.5974773010108642\n",
            "1589th training loss is 0.5974251203972177\n",
            "1590th training loss is 0.5973729484762289\n",
            "1591th training loss is 0.5973207852462327\n",
            "1592th training loss is 0.5972686307055659\n",
            "1593th training loss is 0.597216484852564\n",
            "1594th training loss is 0.5971643476855638\n",
            "1595th training loss is 0.597112219202902\n",
            "1596th training loss is 0.5970600994029156\n",
            "1597th training loss is 0.5970079882839425\n",
            "1598th training loss is 0.5969558858443195\n",
            "1599th training loss is 0.5969037920823843\n",
            "1600th training loss is 0.5968517069964754\n",
            "1601th training loss is 0.596799630584931\n",
            "1602th training loss is 0.5967475628460891\n",
            "1603th training loss is 0.5966955037782891\n",
            "1604th training loss is 0.5966434533798693\n",
            "1605th training loss is 0.5965914116491698\n",
            "1606th training loss is 0.5965393785845301\n",
            "1607th training loss is 0.596487354184289\n",
            "1608th training loss is 0.5964353384467874\n",
            "1609th training loss is 0.5963833313703656\n",
            "1610th training loss is 0.5963313329533635\n",
            "1611th training loss is 0.5962793431941223\n",
            "1612th training loss is 0.5962273620909829\n",
            "1613th training loss is 0.5961753896422871\n",
            "1614th training loss is 0.5961234258463758\n",
            "1615th training loss is 0.5960714707015906\n",
            "1616th training loss is 0.5960195242062744\n",
            "1617th training loss is 0.5959675863587691\n",
            "1618th training loss is 0.5959156571574168\n",
            "1619th training loss is 0.595863736600561\n",
            "1620th training loss is 0.5958118246865446\n",
            "1621th training loss is 0.59575992141371\n",
            "1622th training loss is 0.595708026780402\n",
            "1623th training loss is 0.5956561407849639\n",
            "1624th training loss is 0.5956042634257402\n",
            "1625th training loss is 0.5955523947010739\n",
            "1626th training loss is 0.595500534609311\n",
            "1627th training loss is 0.5954486831487956\n",
            "1628th training loss is 0.595396840317873\n",
            "1629th training loss is 0.5953450061148877\n",
            "1630th training loss is 0.5952931805381868\n",
            "1631th training loss is 0.5952413635861151\n",
            "1632th training loss is 0.5951895552570188\n",
            "1633th training loss is 0.5951377555492442\n",
            "1634th training loss is 0.595085964461138\n",
            "1635th training loss is 0.5950341819910473\n",
            "1636th training loss is 0.5949824081373184\n",
            "1637th training loss is 0.5949306428982996\n",
            "1638th training loss is 0.5948788862723379\n",
            "1639th training loss is 0.5948271382577811\n",
            "1640th training loss is 0.5947753988529774\n",
            "1641th training loss is 0.5947236680562753\n",
            "1642th training loss is 0.5946719458660233\n",
            "1643th training loss is 0.5946202322805703\n",
            "1644th training loss is 0.5945685272982651\n",
            "1645th training loss is 0.5945168309174576\n",
            "1646th training loss is 0.5944651431364967\n",
            "1647th training loss is 0.5944134639537328\n",
            "1648th training loss is 0.5943617933675159\n",
            "1649th training loss is 0.5943101313761965\n",
            "1650th training loss is 0.594258477978125\n",
            "1651th training loss is 0.5942068331716523\n",
            "1652th training loss is 0.5941551969551296\n",
            "1653th training loss is 0.5941035693269079\n",
            "1654th training loss is 0.5940519502853395\n",
            "1655th training loss is 0.5940003398287756\n",
            "1656th training loss is 0.5939487379555686\n",
            "1657th training loss is 0.5938971446640714\n",
            "1658th training loss is 0.5938455599526359\n",
            "1659th training loss is 0.5937939838196153\n",
            "1660th training loss is 0.5937424162633627\n",
            "1661th training loss is 0.5936908572822318\n",
            "1662th training loss is 0.5936393068745752\n",
            "1663th training loss is 0.5935877650387483\n",
            "1664th training loss is 0.5935362317731044\n",
            "1665th training loss is 0.5934847070759978\n",
            "1666th training loss is 0.5934331909457833\n",
            "1667th training loss is 0.5933816833808166\n",
            "1668th training loss is 0.5933301843794514\n",
            "1669th training loss is 0.5932786939400445\n",
            "1670th training loss is 0.5932272120609506\n",
            "1671th training loss is 0.5931757387405264\n",
            "1672th training loss is 0.5931242739771272\n",
            "1673th training loss is 0.5930728177691102\n",
            "1674th training loss is 0.5930213701148321\n",
            "1675th training loss is 0.592969931012649\n",
            "1676th training loss is 0.5929185004609191\n",
            "1677th training loss is 0.5928670784579995\n",
            "1678th training loss is 0.5928156650022472\n",
            "1679th training loss is 0.5927642600920212\n",
            "1680th training loss is 0.5927128637256792\n",
            "1681th training loss is 0.5926614759015801\n",
            "1682th training loss is 0.592610096618082\n",
            "1683th training loss is 0.5925587258735439\n",
            "1684th training loss is 0.5925073636663253\n",
            "1685th training loss is 0.5924560099947856\n",
            "1686th training loss is 0.5924046648572846\n",
            "1687th training loss is 0.5923533282521827\n",
            "1688th training loss is 0.5923020001778393\n",
            "1689th training loss is 0.5922506806326151\n",
            "1690th training loss is 0.5921993696148711\n",
            "1691th training loss is 0.5921480671229683\n",
            "1692th training loss is 0.5920967731552679\n",
            "1693th training loss is 0.5920454877101314\n",
            "1694th training loss is 0.5919942107859202\n",
            "1695th training loss is 0.5919429423809972\n",
            "1696th training loss is 0.591891682493724\n",
            "1697th training loss is 0.5918404311224627\n",
            "1698th training loss is 0.5917891882655775\n",
            "1699th training loss is 0.5917379539214301\n",
            "1700th training loss is 0.5916867280883842\n",
            "1701th training loss is 0.5916355107648031\n",
            "1702th training loss is 0.591584301949051\n",
            "1703th training loss is 0.5915331016394921\n",
            "1704th training loss is 0.5914819098344902\n",
            "1705th training loss is 0.5914307265324102\n",
            "1706th training loss is 0.5913795517316166\n",
            "1707th training loss is 0.5913283854304747\n",
            "1708th training loss is 0.5912772276273496\n",
            "1709th training loss is 0.5912260783206073\n",
            "1710th training loss is 0.5911749375086132\n",
            "1711th training loss is 0.5911238051897335\n",
            "1712th training loss is 0.5910726813623346\n",
            "1713th training loss is 0.591021566024783\n",
            "1714th training loss is 0.5909704591754453\n",
            "1715th training loss is 0.5909193608126891\n",
            "1716th training loss is 0.5908682709348813\n",
            "1717th training loss is 0.5908171895403903\n",
            "1718th training loss is 0.5907661166275832\n",
            "1719th training loss is 0.5907150521948281\n",
            "1720th training loss is 0.5906639962404936\n",
            "1721th training loss is 0.5906129487629482\n",
            "1722th training loss is 0.5905619097605616\n",
            "1723th training loss is 0.5905108792317012\n",
            "1724th training loss is 0.5904598571747379\n",
            "1725th training loss is 0.5904088435880409\n",
            "1726th training loss is 0.5903578384699795\n",
            "1727th training loss is 0.5903068418189248\n",
            "1728th training loss is 0.5902558536332472\n",
            "1729th training loss is 0.5902048739113163\n",
            "1730th training loss is 0.5901539026515035\n",
            "1731th training loss is 0.5901029398521808\n",
            "1732th training loss is 0.5900519855117181\n",
            "1733th training loss is 0.5900010396284888\n",
            "1734th training loss is 0.5899501022008633\n",
            "1735th training loss is 0.5898991732272149\n",
            "1736th training loss is 0.5898482527059155\n",
            "1737th training loss is 0.5897973406353371\n",
            "1738th training loss is 0.5897464370138544\n",
            "1739th training loss is 0.5896955418398393\n",
            "1740th training loss is 0.5896446551116655\n",
            "1741th training loss is 0.5895937768277068\n",
            "1742th training loss is 0.5895429069863374\n",
            "1743th training loss is 0.589492045585931\n",
            "1744th training loss is 0.5894411926248625\n",
            "1745th training loss is 0.5893903481015069\n",
            "1746th training loss is 0.5893395120142383\n",
            "1747th training loss is 0.5892886843614329\n",
            "1748th training loss is 0.5892378651414655\n",
            "1749th training loss is 0.5891870543527118\n",
            "1750th training loss is 0.5891362519935487\n",
            "1751th training loss is 0.5890854580623517\n",
            "1752th training loss is 0.5890346725574974\n",
            "1753th training loss is 0.5889838954773629\n",
            "1754th training loss is 0.588933126820325\n",
            "1755th training loss is 0.5888823665847609\n",
            "1756th training loss is 0.5888316147690484\n",
            "1757th training loss is 0.5887808713715651\n",
            "1758th training loss is 0.5887301363906892\n",
            "1759th training loss is 0.5886794098247987\n",
            "1760th training loss is 0.5886286916722724\n",
            "1761th training loss is 0.5885779819314891\n",
            "1762th training loss is 0.5885272806008282\n",
            "1763th training loss is 0.5884765876786684\n",
            "1764th training loss is 0.5884259031633896\n",
            "1765th training loss is 0.5883752270533718\n",
            "1766th training loss is 0.5883245593469948\n",
            "1767th training loss is 0.5882739000426394\n",
            "1768th training loss is 0.5882232491386851\n",
            "1769th training loss is 0.5881726066335143\n",
            "1770th training loss is 0.5881219725255068\n",
            "1771th training loss is 0.5880713468130453\n",
            "1772th training loss is 0.58802072949451\n",
            "1773th training loss is 0.5879701205682837\n",
            "1774th training loss is 0.587919520032749\n",
            "1775th training loss is 0.5878689278862863\n",
            "1776th training loss is 0.5878183441272802\n",
            "1777th training loss is 0.5877677687541129\n",
            "1778th training loss is 0.5877172017651673\n",
            "1779th training loss is 0.5876666431588276\n",
            "1780th training loss is 0.5876160929334767\n",
            "1781th training loss is 0.5875655510874991\n",
            "1782th training loss is 0.5875150176192786\n",
            "1783th training loss is 0.5874644925271996\n",
            "1784th training loss is 0.5874139758096474\n",
            "1785th training loss is 0.5873634674650066\n",
            "1786th training loss is 0.5873129674916617\n",
            "1787th training loss is 0.5872624758879991\n",
            "1788th training loss is 0.5872119926524044\n",
            "1789th training loss is 0.5871615177832631\n",
            "1790th training loss is 0.5871110512789618\n",
            "1791th training loss is 0.587060593137887\n",
            "1792th training loss is 0.5870101433584253\n",
            "1793th training loss is 0.5869597019389637\n",
            "1794th training loss is 0.5869092688778897\n",
            "1795th training loss is 0.5868588441735907\n",
            "1796th training loss is 0.586808427824454\n",
            "1797th training loss is 0.5867580198288682\n",
            "1798th training loss is 0.5867076201852215\n",
            "1799th training loss is 0.5866572288919024\n",
            "1800th training loss is 0.5866068459472995\n",
            "1801th training loss is 0.586556471349802\n",
            "1802th training loss is 0.5865061050977995\n",
            "1803th training loss is 0.5864557471896812\n",
            "1804th training loss is 0.5864053976238364\n",
            "1805th training loss is 0.5863550563986563\n",
            "1806th training loss is 0.5863047235125303\n",
            "1807th training loss is 0.5862543989638497\n",
            "1808th training loss is 0.5862040827510051\n",
            "1809th training loss is 0.5861537748723876\n",
            "1810th training loss is 0.586103475326388\n",
            "1811th training loss is 0.5860531841113985\n",
            "1812th training loss is 0.586002901225811\n",
            "1813th training loss is 0.5859526266680176\n",
            "1814th training loss is 0.58590236043641\n",
            "1815th training loss is 0.585852102529382\n",
            "1816th training loss is 0.5858018529453259\n",
            "1817th training loss is 0.5857516116826346\n",
            "1818th training loss is 0.5857013787397012\n",
            "1819th training loss is 0.5856511541149209\n",
            "1820th training loss is 0.5856009378066862\n",
            "1821th training loss is 0.5855507298133917\n",
            "1822th training loss is 0.5855005301334318\n",
            "1823th training loss is 0.5854503387652015\n",
            "1824th training loss is 0.5854001557070954\n",
            "1825th training loss is 0.5853499809575083\n",
            "1826th training loss is 0.5852998145148366\n",
            "1827th training loss is 0.5852496563774757\n",
            "1828th training loss is 0.5851995065438211\n",
            "1829th training loss is 0.5851493650122696\n",
            "1830th training loss is 0.5850992317812171\n",
            "1831th training loss is 0.5850491068490612\n",
            "1832th training loss is 0.5849989902141981\n",
            "1833th training loss is 0.5849488818750255\n",
            "1834th training loss is 0.5848987818299403\n",
            "1835th training loss is 0.5848486900773411\n",
            "1836th training loss is 0.584798606615626\n",
            "1837th training loss is 0.5847485314431918\n",
            "1838th training loss is 0.5846984645584391\n",
            "1839th training loss is 0.5846484059597651\n",
            "1840th training loss is 0.5845983556455697\n",
            "1841th training loss is 0.5845483136142521\n",
            "1842th training loss is 0.5844982798642117\n",
            "1843th training loss is 0.584448254393848\n",
            "1844th training loss is 0.5843982372015615\n",
            "1845th training loss is 0.5843482282857524\n",
            "1846th training loss is 0.5842982276448215\n",
            "1847th training loss is 0.5842482352771697\n",
            "1848th training loss is 0.5841982511811982\n",
            "1849th training loss is 0.5841482753553072\n",
            "1850th training loss is 0.5840983077979\n",
            "1851th training loss is 0.5840483485073771\n",
            "1852th training loss is 0.5839983974821418\n",
            "1853th training loss is 0.5839484547205953\n",
            "1854th training loss is 0.5838985202211415\n",
            "1855th training loss is 0.5838485939821824\n",
            "1856th training loss is 0.5837986760021213\n",
            "1857th training loss is 0.5837487662793622\n",
            "1858th training loss is 0.5836988648123079\n",
            "1859th training loss is 0.5836489715993632\n",
            "1860th training loss is 0.5835990866389318\n",
            "1861th training loss is 0.5835492099294182\n",
            "1862th training loss is 0.5834993414692273\n",
            "1863th training loss is 0.5834494812567639\n",
            "1864th training loss is 0.5833996292904331\n",
            "1865th training loss is 0.5833497855686408\n",
            "1866th training loss is 0.5832999500897922\n",
            "1867th training loss is 0.5832501228522937\n",
            "1868th training loss is 0.5832003038545515\n",
            "1869th training loss is 0.5831504930949716\n",
            "1870th training loss is 0.5831006905719616\n",
            "1871th training loss is 0.5830508962839283\n",
            "1872th training loss is 0.5830011102292786\n",
            "1873th training loss is 0.5829513324064204\n",
            "1874th training loss is 0.5829015628137612\n",
            "1875th training loss is 0.5828518014497092\n",
            "1876th training loss is 0.5828020483126727\n",
            "1877th training loss is 0.5827523034010602\n",
            "1878th training loss is 0.5827025667132809\n",
            "1879th training loss is 0.5826528382477437\n",
            "1880th training loss is 0.5826031180028572\n",
            "1881th training loss is 0.5825534059770325\n",
            "1882th training loss is 0.5825037021686782\n",
            "1883th training loss is 0.5824540065762048\n",
            "1884th training loss is 0.582404319198023\n",
            "1885th training loss is 0.5823546400325426\n",
            "1886th training loss is 0.5823049690781753\n",
            "1887th training loss is 0.5822553063333319\n",
            "1888th training loss is 0.5822056517964236\n",
            "1889th training loss is 0.5821560054658631\n",
            "1890th training loss is 0.5821063673400612\n",
            "1891th training loss is 0.5820567374174302\n",
            "1892th training loss is 0.5820071156963829\n",
            "1893th training loss is 0.5819575021753322\n",
            "1894th training loss is 0.5819078968526902\n",
            "1895th training loss is 0.5818582997268704\n",
            "1896th training loss is 0.581808710796287\n",
            "1897th training loss is 0.5817591300593536\n",
            "1898th training loss is 0.5817095575144827\n",
            "1899th training loss is 0.5816599931600904\n",
            "1900th training loss is 0.58161043699459\n",
            "1901th training loss is 0.5815608890163972\n",
            "1902th training loss is 0.5815113492239261\n",
            "1903th training loss is 0.5814618176155923\n",
            "1904th training loss is 0.5814122941898116\n",
            "1905th training loss is 0.5813627789449993\n",
            "1906th training loss is 0.5813132718795719\n",
            "1907th training loss is 0.5812637729919455\n",
            "1908th training loss is 0.5812142822805365\n",
            "1909th training loss is 0.5811647997437625\n",
            "1910th training loss is 0.5811153253800392\n",
            "1911th training loss is 0.5810658591877852\n",
            "1912th training loss is 0.5810164011654173\n",
            "1913th training loss is 0.580966951311354\n",
            "1914th training loss is 0.5809175096240128\n",
            "1915th training loss is 0.5808680761018121\n",
            "1916th training loss is 0.5808186507431711\n",
            "1917th training loss is 0.5807692335465083\n",
            "1918th training loss is 0.5807198245102426\n",
            "1919th training loss is 0.5806704236327942\n",
            "1920th training loss is 0.5806210309125817\n",
            "1921th training loss is 0.5805716463480255\n",
            "1922th training loss is 0.5805222699375461\n",
            "1923th training loss is 0.5804729016795634\n",
            "1924th training loss is 0.5804235415724986\n",
            "1925th training loss is 0.5803741896147722\n",
            "1926th training loss is 0.5803248458048059\n",
            "1927th training loss is 0.5802755101410203\n",
            "1928th training loss is 0.580226182621838\n",
            "1929th training loss is 0.5801768632456801\n",
            "1930th training loss is 0.5801275520109703\n",
            "1931th training loss is 0.5800782489161297\n",
            "1932th training loss is 0.5800289539595813\n",
            "1933th training loss is 0.5799796671397489\n",
            "1934th training loss is 0.5799303884550551\n",
            "1935th training loss is 0.5798811179039232\n",
            "1936th training loss is 0.5798318554847769\n",
            "1937th training loss is 0.5797826011960413\n",
            "1938th training loss is 0.5797333550361401\n",
            "1939th training loss is 0.5796841170034978\n",
            "1940th training loss is 0.5796348870965392\n",
            "1941th training loss is 0.5795856653136894\n",
            "1942th training loss is 0.5795364516533741\n",
            "1943th training loss is 0.5794872461140184\n",
            "1944th training loss is 0.5794380486940481\n",
            "1945th training loss is 0.5793888593918896\n",
            "1946th training loss is 0.5793396782059695\n",
            "1947th training loss is 0.5792905051347138\n",
            "1948th training loss is 0.57924134017655\n",
            "1949th training loss is 0.5791921833299047\n",
            "1950th training loss is 0.5791430345932062\n",
            "1951th training loss is 0.579093893964881\n",
            "1952th training loss is 0.5790447614433573\n",
            "1953th training loss is 0.5789956370270641\n",
            "1954th training loss is 0.5789465207144294\n",
            "1955th training loss is 0.5788974125038814\n",
            "1956th training loss is 0.5788483123938494\n",
            "1957th training loss is 0.5787992203827631\n",
            "1958th training loss is 0.5787501364690514\n",
            "1959th training loss is 0.578701060651144\n",
            "1960th training loss is 0.5786519929274712\n",
            "1961th training loss is 0.5786029332964628\n",
            "1962th training loss is 0.5785538817565499\n",
            "1963th training loss is 0.5785048383061631\n",
            "1964th training loss is 0.5784558029437331\n",
            "1965th training loss is 0.5784067756676914\n",
            "1966th training loss is 0.5783577564764695\n",
            "1967th training loss is 0.5783087453684994\n",
            "1968th training loss is 0.578259742342213\n",
            "1969th training loss is 0.5782107473960426\n",
            "1970th training loss is 0.5781617605284205\n",
            "1971th training loss is 0.5781127817377801\n",
            "1972th training loss is 0.5780638110225539\n",
            "1973th training loss is 0.5780148483811759\n",
            "1974th training loss is 0.5779658938120794\n",
            "1975th training loss is 0.577916947313698\n",
            "1976th training loss is 0.5778680088844664\n",
            "1977th training loss is 0.577819078522819\n",
            "1978th training loss is 0.5777701562271897\n",
            "1979th training loss is 0.5777212419960137\n",
            "1980th training loss is 0.5776723358277263\n",
            "1981th training loss is 0.5776234377207631\n",
            "1982th training loss is 0.5775745476735595\n",
            "1983th training loss is 0.5775256656845518\n",
            "1984th training loss is 0.5774767917521756\n",
            "1985th training loss is 0.577427925874868\n",
            "1986th training loss is 0.5773790680510651\n",
            "1987th training loss is 0.5773302182792048\n",
            "1988th training loss is 0.5772813765577232\n",
            "1989th training loss is 0.5772325428850588\n",
            "1990th training loss is 0.5771837172596483\n",
            "1991th training loss is 0.5771348996799307\n",
            "1992th training loss is 0.5770860901443435\n",
            "1993th training loss is 0.577037288651326\n",
            "1994th training loss is 0.5769884951993165\n",
            "1995th training loss is 0.5769397097867546\n",
            "1996th training loss is 0.5768909324120786\n",
            "1997th training loss is 0.5768421630737289\n",
            "1998th training loss is 0.5767934017701444\n",
            "1999th training loss is 0.5767446484997664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "def decision_region(X,y,slr):\n",
        "    mesh_f0, mesh_f1  = np.meshgrid(\n",
        "        np.arange(np.min(X[:,0]), np.max(X[:,0]), 0.01), \n",
        "        np.arange(np.min(X[:,1]), np.max(X[:,1]), 0.01)\n",
        "    )\n",
        "    mesh = np.c_[np.ravel(mesh_f0),np.ravel(mesh_f1)]\n",
        "    y_pred = slr.predict(mesh).reshape(mesh_f0.shape)\n",
        "    plt.title('decision region')\n",
        "    plt.xlabel('feature0')\n",
        "    plt.ylabel('feature1')\n",
        "    plt.contourf(mesh_f0, mesh_f1, y_pred,cmap=ListedColormap(['pink', 'skyblue']))\n",
        "    plt.contour(mesh_f0, mesh_f1, y_pred,colors='red')\n",
        "    plt.scatter(X[y==0][:, 0], X[y==0][:, 1],label='0')\n",
        "    plt.scatter(X[y==1][:, 0], X[y==1][:, 1],label='1')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "HAoZOl4Js4aR"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decision_region(X,y,slr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "ifeUqWGys84b",
        "outputId": "ec2b9956-2e8d-4ad3-d231-30d81c65828b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}