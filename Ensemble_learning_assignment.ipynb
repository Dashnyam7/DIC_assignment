{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMtg/8mJkXOg9OJFPTCvw7E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dashnyam7/DIC_assignment/blob/main/Ensemble_learning_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "L94ZKNuW7jbF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "dataset = pd.read_csv(\"train.csv\")\n",
        "X = dataset.loc[:, ['GrLivArea', 'YearBuilt']]\n",
        "y = dataset.loc[:, ['SalePrice']]\n",
        "X = X.values\n",
        "y = y.values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.fit_transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Problem 1] Scratch implementation of blending"
      ],
      "metadata": {
        "id": "dSWWgGVgE7Kz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is blending"
      ],
      "metadata": {
        "id": "y2BIQI-nFGni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Blending is a method of independently learning N diverse models, weighting the estimation results, and adding them together. The simplest is to take the average. Diverse models are created by changing the following conditions.\n",
        "\n",
        "-Techniques (e.g. linear regression, SVM, decision trees, neural networks, etc.)\n",
        "\n",
        "-Hyperparameters (e.g. SVM kernel type, initial weights, etc.)\n",
        "\n",
        "-How to preprocess the input data (e.g. normalization, logarithmic transformation, PCA, etc.)\n",
        "Importantly, each model is very different.\n",
        "\n",
        "Blending for regression problems is so simple that scikit-learn does not provide it."
      ],
      "metadata": {
        "id": "1kT3t1YIFJIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For classification problems, majority vote is used. VotingClassifier is provided in scikit-learn because it is more complicated than regression problems."
      ],
      "metadata": {
        "id": "AFGaLPRlFBn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LinearRegression\n",
        "svc = SVC() \n",
        "rf = RandomForestClassifier()\n",
        "tree = DecisionTreeClassifier(max_depth=3)\n",
        "vc = VotingClassifier(estimators=[('svc', svc), ('rf', rf), ('tree', tree)], voting='hard')\n",
        "vc = vc.fit(X_train, y_train)\n",
        "pred = vc.predict(X_test)\n",
        "pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmkuMD67Fa7V",
        "outputId": "21dcfdea-6aa5-4b73-c8ad-cf2759f949b2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([173000, 100000, 178000, 100000, 178000, 136000, 178000, 109000,\n",
              "        80000, 160000, 178000, 100000, 178000, 178000, 178000, 135000,\n",
              "       100000,  80000, 147400, 178000, 178000, 159895, 178000, 100000,\n",
              "        80000, 178000, 109000, 140000, 100000, 140000, 135000, 135000,\n",
              "       140000, 178000, 100000, 178000, 140000, 100000, 172500, 178000,\n",
              "       178000, 140000, 140000, 135000, 178000, 140000, 140000, 178000,\n",
              "       140000, 190000, 123500, 100000, 178000, 100000, 178000, 178000,\n",
              "       135000, 178000, 135000, 178000, 140000, 178000, 100000, 135000,\n",
              "       178000, 135000, 178000, 100000, 178000, 140000, 140000, 178000,\n",
              "       140000, 135000, 135000, 178000, 140000, 100000, 140000, 178000,\n",
              "       178000, 100000, 140000, 140000, 135000, 164000, 135000, 135000,\n",
              "       135000, 178000, 135000, 178000, 178000,  82000, 135000, 140000,\n",
              "       140000, 178000, 130500, 178000, 140000, 178000, 178000, 130500,\n",
              "       178000, 136000, 135000, 178000,  99500, 140000, 140000, 140000,\n",
              "       100000, 178000, 100000, 178000, 135000, 178000, 178000, 178000,\n",
              "       140000, 178000, 140000, 117500, 178000, 178000, 158000, 178000,\n",
              "       144000, 115000, 178000, 100000, 144000, 100000, 178000, 140000,\n",
              "       178000, 100000, 178000, 100000, 143000, 123500, 140000, 155000,\n",
              "       178000, 175000, 140000, 135000, 178000, 178000, 100000, 100000,\n",
              "        82000,  89471, 178000, 140000, 178000, 178000, 178000, 140000,\n",
              "       178000, 178000, 178000, 100000, 110000, 178000, 178000, 129500,\n",
              "       177500, 178000, 178000, 140000, 185000, 178000, 140000, 178000,\n",
              "       185000, 178000, 178000, 100000, 140000, 178000, 140000, 153500,\n",
              "       178000, 178000, 135000, 100000, 178000, 100000, 100000, 178000,\n",
              "       135000, 140000, 190000, 100000, 100000, 100000, 178000, 129900,\n",
              "        80000, 140000, 174000, 135000, 178000, 140000, 100000, 190000,\n",
              "       178000, 100000, 140000, 121600, 100000, 140000, 140000, 140000,\n",
              "       178000, 140000, 135000, 140000, 178000, 178000, 140000, 100000,\n",
              "       178000, 126500, 140000, 119200, 100000, 131000, 100000, 135000,\n",
              "       106000, 190000, 178000, 100000, 100000, 135000, 178000, 119200,\n",
              "       178000, 140000, 178000, 190000, 135000, 190000, 178000, 140000,\n",
              "       140000, 174000, 178000, 178000, 178000, 122000, 140000, 178000,\n",
              "       178000, 178000, 130500, 178000, 140000, 120000, 100000, 100000,\n",
              "       136000,  93000, 119200, 178000, 178000, 178000, 129900, 190000,\n",
              "       100000, 156932, 178000, 100000, 178000, 140000, 147000,  67000,\n",
              "       140000, 136000, 140000, 100000, 100000, 140000, 134000, 190000,\n",
              "       100000, 100000, 178000, 140000])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(y_test, pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfgM12M1LU_c",
        "outputId": "409f63bb-440b-4e8c-e1e3-0410d9f72c46"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.003424657534246575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tree.fit(X_train, y_train)\n",
        "pred2 = tree.predict(X_test)\n",
        "print(accuracy_score(y_test, pred2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goYXToKrL0gT",
        "outputId": "2e0bb5e5-de8b-4dce-c1d9-018a3a29837b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.010273972602739725\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Problem 2] Scratch implementation of bagging"
      ],
      "metadata": {
        "id": "ZwZt4ahFFGqj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is bagging"
      ],
      "metadata": {
        "id": "elAZo3APKhQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bagging is a method of diversifying the selection of input data. Create N types of subsets ( bootstrap samples ) by randomly extracting from the training data while allowing duplication . Train N models with them and average the estimation results. Unlike blending, it does not change the weighting of each."
      ],
      "metadata": {
        "id": "QtWkD7TeKk8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "regr = BaggingRegressor(base_estimator=svc,n_estimators=10, random_state=0).fit(X_train, y_train)\n",
        "pred3 = regr.predict(X_test)\n",
        "pred3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qo2dpa0dOaAv",
        "outputId": "1642ce59-a1a5-4d15-ba89-f40dc0ad785c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/ensemble/_bagging.py:429: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  return column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([147550. , 126610. , 158320. , 104547.1, 213180. , 149379. ,\n",
              "       182349. , 112682.1, 102907.1, 123980. , 182349. ,  83964.3,\n",
              "       167540. , 178639. , 182349. , 126930. , 107867.1, 103767.1,\n",
              "       158595. , 175524. , 173389. , 158595. , 158530. , 100442.1,\n",
              "       125190. , 205215. , 120790. , 123685. ,  93024.3, 137135. ,\n",
              "       127830. , 125330. , 128365. , 177148. ,  97362.1, 156180. ,\n",
              "       122960. , 100442.1, 162370. , 178639. , 158530. , 157410. ,\n",
              "       143910. , 127830. , 182349. , 132180. , 130635. , 177314. ,\n",
              "       137135. , 163125.3, 125330. , 104547.1, 146825. ,  83964.3,\n",
              "       182349. , 190274. , 127830. , 190274. , 115270. , 158595. ,\n",
              "       146210. , 183046.5, 110167.1, 120790. , 182349. , 156280. ,\n",
              "       167270. , 125190. , 188218. , 134028.2, 129385. , 156180. ,\n",
              "       138020. , 125330. , 126440.4, 182349. , 128785. , 107867.1,\n",
              "       138480. , 184948. , 151040. , 117917.1, 164970. , 127365. ,\n",
              "       127830. , 176989. , 129830. , 126930. , 127830. , 158530. ,\n",
              "       125330. , 201090. , 210290. , 114597.1, 126930. , 128958.2,\n",
              "       148645. , 178639. , 119860.4, 182349. , 129385. , 177148. ,\n",
              "       179448. , 147965. , 223105. , 161080. , 129830. , 178639. ,\n",
              "       127890.4, 128760. , 125330. , 127365. , 119007.5, 197115. ,\n",
              "        74934.3, 151440. , 126930. , 188218. , 182349. , 178639. ,\n",
              "       154830. , 187576.5, 133585. , 114652.5, 178639. , 179949. ,\n",
              "       179630. , 178639. , 144715. , 151245. , 156180. , 124540. ,\n",
              "       144715. , 102677.9, 158910. , 134485. , 192614. , 127180. ,\n",
              "       203830. , 104547.1, 154280. , 125330. , 121710. , 147950. ,\n",
              "       178639. , 155730. , 143160. , 127830. , 179949. , 211880. ,\n",
              "       113357.5,  93112.1, 100272.1, 103575. , 182349. , 126930. ,\n",
              "       187576.5, 167164. , 164920. , 126885. , 174689. , 167164. ,\n",
              "       182349. , 120717.1, 125630. , 182349. , 218320. , 126440.4,\n",
              "       162770. , 182349. , 182349. , 128715. , 157595. , 178639. ,\n",
              "       119700. , 178639. , 158530. , 188218. , 157345. , 125060.4,\n",
              "       123685. , 228723.7, 149425. , 157700. , 156945. , 178639. ,\n",
              "       125330. , 104042.1, 211765. , 100882.1, 117917.1, 147950. ,\n",
              "       126930. , 132085. , 198840. ,  87715. , 126440.4, 115447.1,\n",
              "       198265. , 127290. , 125190. , 126555. , 167164. , 126440.4,\n",
              "       209640. , 124975. , 125190. , 190050. , 196065. , 119007.5,\n",
              "       119435. , 125630. , 119007.5, 130165. , 121560. , 138510. ,\n",
              "       206430. , 125935. , 125330. , 133585. , 158530. , 158530. ,\n",
              "       127665. ,  91600.8, 167164. , 127830. , 136935. , 115185. ,\n",
              "       117367.1, 124630. , 114597.1, 124400.4, 117025. , 198840. ,\n",
              "       158310. ,  88665. , 117937.1, 125330. , 182349. , 113532.1,\n",
              "       174689. , 127235. , 219438.7, 169290. , 121937.1, 198840. ,\n",
              "       157700. , 164970. , 121470. , 167164. , 178348. , 192614. ,\n",
              "       198840. , 126130. , 129385. , 178639. , 213180. , 162770. ,\n",
              "       154645. , 182349. , 129385. , 123430. , 117917.1,  93775. ,\n",
              "       149379. , 123430. , 117100. , 189368. , 158530. , 179949. ,\n",
              "       125290. , 157950. ,  91964.3, 158530. , 223105. , 114597.1,\n",
              "       157090. , 128715. , 182349. ,  94122.1, 128958.2, 162680. ,\n",
              "       142050. , 121807.5,  87715. , 139870. , 134100. , 194790. ,\n",
              "       117367.1, 109132.1, 158310. , 128958.2])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Problem 3] Scratch implementation of stacking"
      ],
      "metadata": {
        "id": "PnnASguXPm3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is stacking"
      ],
      "metadata": {
        "id": "MHtxLpwtPp_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The stacking procedure is as follows. Stacking is valid if there are at least stage 0 and stage 1, so please implement it. First\n",
        "$K_0$=3, $M_0$=2\n",
        "to a degree."
      ],
      "metadata": {
        "id": "RP5VwOzLPstc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 《When learning》"
      ],
      "metadata": {
        "id": "EGsRTUdRQH3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(stage $0$ )\n",
        "\n",
        "-Split the training data into $K_0$ pieces.\n",
        "\n",
        "-You can create $K_0$ combinations of $(K_0 - 1)$ splits as training data and the remaining $1$ as estimation data.\n",
        "\n",
        "-Prepare $K_0$ instances of a model and train it using different training data.\n",
        "\n",
        "-For each trained model, input the remaining $1$ data for estimation that are not used and obtain estimated values. (This is called blend data)\n",
        "\n",
        "-In addition, prepare $K_0$ instances of different models and do the same thing. If you have $M_0$ models, you will get $M_0$ blended data."
      ],
      "metadata": {
        "id": "pr35eKGkP9-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(stage $n$ )\n",
        "\n",
        "-Consider the blended data of stage $n-1$ as training data with $M_{n-1}$-dimensional features, and divide it into $K_n$ pieces. The same applies hereinafter.\n",
        "(Stage $N$ ) * Last stage\n",
        "\n",
        "-$M_{N-1}$ pieces of blended data of stage $N-1$ are used as inputs for $M_{N-1}$ dimensional features to train one type of model. This becomes the model for the final estimation."
      ],
      "metadata": {
        "id": "hhSeoUCCQVZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### \"Presumption\""
      ],
      "metadata": {
        "id": "gGY_5uJfQH6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(stage $0$ )\n",
        "\n",
        "Input test data into $K_0×M_0$ trained models and obtain $K_0×M_0$ estimates. Calculate the average value of this on the $K_0$ axis to obtain data with $M_0$-dimensional feature values. (called blend test)\n",
        "\n",
        "\n",
        "(stage $n$ )\n",
        "\n",
        "We input the blended tests obtained in stage $n-1$ into $K_n×M_n$ trained models and obtain $K_n×M_n$ estimates. Calculate the average value of this on the $K_n$ axis to obtain data with $M_0$-dimensional feature values. (called blend test)\n",
        "\n",
        "\n",
        "(Stage $N$ ) * Last stage\n",
        "\n",
        "Input the blended test obtained in stage $N-1$ into the trained model and get an estimate."
      ],
      "metadata": {
        "id": "BMQttUanQcTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingRegressor, RandomForestRegressor\n",
        "sr = StackingRegressor(estimators=[('lr', lr)], final_estimator=RandomForestRegressor())\n",
        "sr = sr.fit(X_train, y_train)\n",
        "pred4 = sr.predict(X_test)\n",
        "pred4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGlQFcYoQkqo",
        "outputId": "e1e68fff-d230-4259-d48d-c7d586cf5443"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/ensemble/_stacking.py:758: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([142827.64      , 140077.25      , 151424.        , 115609.        ,\n",
              "       341042.21      , 217448.86      , 249586.06      , 110901.25      ,\n",
              "       107035.        , 141879.46      , 233415.        ,  90129.        ,\n",
              "       180560.6       , 239486.4       , 215475.        , 133990.        ,\n",
              "       114970.        , 123308.5       , 178800.        , 265679.        ,\n",
              "       195155.        , 140000.        , 186849.        , 112131.        ,\n",
              "       132818.25      , 412750.        , 110547.        , 134629.        ,\n",
              "       112994.        , 134730.        , 136350.4       , 146174.        ,\n",
              "       180020.        , 273264.        , 105035.        , 208365.        ,\n",
              "       132965.        , 111776.        , 200174.55      , 239241.        ,\n",
              "       174436.41666667, 152575.        , 165260.        , 116920.        ,\n",
              "       219037.        , 146890.        , 148837.4       , 204809.        ,\n",
              "       152630.        , 179460.        , 155428.        ,  92765.        ,\n",
              "       142594.        ,  90783.        , 255353.37      , 332227.        ,\n",
              "       130261.3       , 233048.88      , 113192.        , 166045.        ,\n",
              "       141002.        , 270507.5       , 117043.        , 121310.20833333,\n",
              "       230794.96      , 188612.75      , 189236.8       , 132195.5       ,\n",
              "       231530.83      , 173490.        , 131800.        , 180560.6       ,\n",
              "       160670.57      , 138399.        , 129938.        , 214415.        ,\n",
              "       160941.66666667,  85282.5       , 155663.        , 257917.68      ,\n",
              "       186790.        , 136220.        , 189000.        , 152323.        ,\n",
              "       137102.75      , 193030.        , 170005.        , 135605.        ,\n",
              "       134043.        , 188417.        ,  99368.        , 325190.        ,\n",
              "       313415.        , 132115.        , 146095.        , 164860.        ,\n",
              "       190758.75      , 228803.5       , 132195.5       , 216930.        ,\n",
              "       163715.        , 285473.        , 245596.48      , 157799.        ,\n",
              "       548325.96      , 181763.01238095, 192480.        , 201220.        ,\n",
              "       135605.4       , 143530.        , 139296.        , 157410.        ,\n",
              "       132022.5       , 281380.        ,  76201.        , 181105.        ,\n",
              "       123560.        , 365545.        , 225077.78      , 288530.        ,\n",
              "       151310.        , 349592.        , 154495.        , 140150.        ,\n",
              "       220920.        , 222545.        , 220857.6       , 234600.87      ,\n",
              "       144388.        , 243073.        , 223432.        , 140307.08333333,\n",
              "       144388.        ,  62824.52666667, 208885.6       , 156780.        ,\n",
              "       281630.        , 118017.        , 307219.        , 115360.16      ,\n",
              "       190758.75      , 139327.75      , 117340.        , 180458.        ,\n",
              "       224635.        , 143685.        , 180665.        , 134595.        ,\n",
              "       279850.        , 410642.28      ,  66835.        , 111776.        ,\n",
              "        92063.5       , 129125.        , 231037.78      , 141650.        ,\n",
              "       270228.5       , 236241.        , 229666.7       , 142319.6       ,\n",
              "       214434.5       , 223855.        , 218854.        , 110010.        ,\n",
              "       149679.5       , 214100.        , 404513.45      , 138509.        ,\n",
              "       222570.        , 298963.52      , 214100.        , 149600.        ,\n",
              "       189080.        , 268268.54      , 121475.        , 239046.25      ,\n",
              "       203395.        , 295330.58      , 174586.5       , 117040.        ,\n",
              "       190145.        , 431080.        , 173490.        , 186475.        ,\n",
              "       183349.        , 184625.        , 163136.        , 107145.        ,\n",
              "       396972.7       ,  87216.5       , 118750.        , 152707.        ,\n",
              "       155745.        , 154350.        , 330805.86      ,  74779.        ,\n",
              "       117935.        , 146790.25      , 269168.37      , 189729.        ,\n",
              "       116915.        , 110468.5       , 274015.01      , 148275.        ,\n",
              "       333542.17      , 142033.        , 132195.5       , 303568.84      ,\n",
              "       321893.72      , 132022.5       , 122158.        , 135258.        ,\n",
              "        83064.15      , 139390.5       ,  98115.25      , 138042.        ,\n",
              "       325820.        , 117140.91666667, 143188.        , 149181.        ,\n",
              "       196146.5       , 183305.        , 150825.        , 103839.        ,\n",
              "       217165.2       , 135530.        , 172220.        , 125251.5       ,\n",
              "       111590.        , 123907.        ,  94649.        , 132514.4       ,\n",
              "       155089.        , 341042.21      , 155340.        ,  45123.88666667,\n",
              "       110537.16      , 105035.        , 245308.96      , 122781.        ,\n",
              "       289600.        , 140520.        , 464775.78      , 274334.24      ,\n",
              "       109505.        , 371073.82      , 158745.5       , 224328.        ,\n",
              "        88016.07      , 196884.93      , 277607.35      , 271130.        ,\n",
              "       404275.25      , 135105.        , 145514.        , 204284.42      ,\n",
              "       292858.38      , 192188.5       , 181105.        , 215329.        ,\n",
              "       122558.        , 139671.        , 118750.        ,  81819.        ,\n",
              "       255476.4       , 128230.        , 102777.        , 189896.        ,\n",
              "       166700.        , 229690.25      , 147040.76      , 223910.        ,\n",
              "        90255.        , 165190.        , 546280.96      , 105615.39      ,\n",
              "       206622.8       , 160110.        , 217430.        ,  84122.        ,\n",
              "       146267.84      , 190090.        , 163820.        , 117010.        ,\n",
              "        53485.5       , 143510.        , 151548.        , 321030.        ,\n",
              "       119910.        , 118207.24      , 213310.        , 170340.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy_score(y_test, pred2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iZVipHMYJm_",
        "outputId": "74302cda-d757-4b1f-bff8-3a9dec10777e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.010273972602739725\n"
          ]
        }
      ]
    }
  ]
}